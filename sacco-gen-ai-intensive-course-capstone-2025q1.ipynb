{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lousacco/sacco-gen-ai-intensive-course-capstone-2025q1?scriptVersionId=235077142\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"8565bbc8","metadata":{"papermill":{"duration":0.004715,"end_time":"2025-04-20T17:32:15.951316","exception":false,"start_time":"2025-04-20T17:32:15.946601","status":"completed"},"tags":[]},"source":["# Reimagining Benefits Enrollment with Google's GenAI Stack\n","\n","## Use Case\n","\n","Employers and insurance providers spend significant time manually entering detailed benefits data into HRIS systems when setting up open enrollment. This data, usually captured in Summary of Benefits and Coverage (SBC) PDF documents, must be painstakingly re-entered, creating tedious and error-prone work. Additionally, employees often find it challenging to choose the most suitable medical, dental, or vision plan because comparing detailed benefits across multiple documents can be confusing. As a result, decisions frequently default to cost rather than overall value.\n","\n","Generative AI (GenAI) can address these pain points effectively by utilizing Retrieval-Augmented Generation (RAG). With a RAG-based system, SBC documents are ingested and embedded into a vector store at plan creation time. Employees can then interact through a user-friendly chat interface during open enrollment, asking specific questions about each plan. The system can dynamically generate comparison tables or personalized recommendations based on individual needs. This project illustrates how leveraging GenAI capabilities learned in this course can significantly simplify and improve benefits plan selection.\n","\n","## Objective\n","This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline leveraging Generative AI (Google Gemini) to extract structured, meaningful data from uploaded SBC PDFs, enabling easy and transparent comparison of insurance plans. Furthermore, this can serve as an example for any industry that relies heavily on PDF documents to share information with its clients. The objective is to illustrate what's possible with this use case that is easily extended to others.\n","\n","### Gen AI Capabilities Demonstrated\n","\n","-   **Embeddings & Vector Store:** Utilized `GoogleGenerativeAIEmbeddings` to create vector representations of SBC document text chunks, storing them in a `Chroma` vector database for efficient retrieval.\n","-   **Vector Search/Vector Store/Vector Database:** Employed `Chroma` as a vector database and performed similarity searches (`store.similarity_search`) to find document chunks relevant to specific queries (including a multi-query approach).\n","-   **Retrieval Augmented Generation (RAG) & Grounding:** Implemented a multi-query RAG pipeline that retrieves relevant document chunks (context) from the vector store and provides this context to an LLM, instructing it to base its answers *only* on this retrieved information, ensuring grounded responses.\n","-   **Document Understanding:** Processed PDF documents (SBCs) using `PyMuPDF`, extracted text content, and used the LLM to interpret this text to extract specific data points based on context.\n","-   **Structured Output (JSON mode/controlled generation):** Engineered detailed prompts, including schema definitions and strict formatting rules, to guide the LLM in generating responses formatted as valid JSON objects containing extracted SBC details.\n","-   **Few-Shot Prompting:** Included a concrete example of the desired JSON output structure within the LLM prompt to improve the accuracy and formatting of the generated response based on the provided context.\n","\n","Let's get started and for any issues don't hesitate to contact me directly through my Kaggle [profile](https://www.kaggle.com/lousacco)."]},{"cell_type":"markdown","id":"1da9297d","metadata":{"papermill":{"duration":0.00374,"end_time":"2025-04-20T17:32:15.95963","exception":false,"start_time":"2025-04-20T17:32:15.95589","status":"completed"},"tags":[]},"source":["## Project Set-up\n","\n","The first step is to set-up the SDK and include the required packages I'll use throughout this notebook. Note that there may be some dependency conflicts, perhaps due to some already installed packages that come with Kaggle. I've tried to eliminate these best as possible by uninstalling them and relying on the newer versions installed. If you encounter them, these are innocuous and do not affect the running of the rest of the project."]},{"cell_type":"code","execution_count":1,"id":"e406c713","metadata":{"execution":{"iopub.execute_input":"2025-04-20T17:32:15.968583Z","iopub.status.busy":"2025-04-20T17:32:15.968277Z","iopub.status.idle":"2025-04-20T17:33:23.215604Z","shell.execute_reply":"2025-04-20T17:33:23.214529Z"},"papermill":{"duration":67.253594,"end_time":"2025-04-20T17:33:23.217106","exception":false,"start_time":"2025-04-20T17:32:15.963512","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n","google-genai version: 1.11.0\n","Successfully imported desired packages.\n"]}],"source":["# Step 1: Uninstall specific packages causing some conflicts\n","!pip uninstall -y -q google-cloud-automl google-cloud-translate gcsfs bigframes google-generativeai\n","\n","# Step 2: Upgrade pip (Good Practice)\n","!pip install -q -U pip\n","\n","# Step 3: Install required packages\n","!pip install -q -U \\\n","    google-genai \\\n","    langchain-community \\\n","    PyMuPDF \\\n","    chromadb \\\n","    langchain-google-genai \\\n","    langchain-chroma \\\n","    tenacity\n","\n","# Step 4: Verify your core imports/functionality\n","try:\n","    import google.genai as genai\n","    print(f\"google-genai version: {genai.__version__}\") # Check version\n","    import langchain_google_genai\n","    import chromadb\n","    print(\"Successfully imported desired packages.\")\n","except ImportError as e:\n","    print(f\"Installation failed or import error: {e}\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")"]},{"cell_type":"markdown","id":"90d7d116","metadata":{"papermill":{"duration":0.004726,"end_time":"2025-04-20T17:33:23.22708","exception":false,"start_time":"2025-04-20T17:33:23.222354","status":"completed"},"tags":[]},"source":["## Account & API Key Set-up\n","\n","To run this code, you will need a validated active Kaggle Account. Follow these steps to get set-up:\n","\n","1. Create a Kaggle account and be sure to phone verify your account (under Profile->Settings). This will give you Internet access from this project, which is required.\n","2. Next click \"Copy & Edit\" in the upper right corner of the project.\n","3. Under Notebook Session Options, be sure \"Internet on\" is checked. Per step 1, you need to be phone verified for this to work.\n","4. To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`. If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n","5. In Kaggle, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."]},{"cell_type":"code","execution_count":2,"id":"f5e43831","metadata":{"execution":{"iopub.execute_input":"2025-04-20T17:33:23.238636Z","iopub.status.busy":"2025-04-20T17:33:23.238153Z","iopub.status.idle":"2025-04-20T17:33:23.405666Z","shell.execute_reply":"2025-04-20T17:33:23.404514Z"},"papermill":{"duration":0.17529,"end_time":"2025-04-20T17:33:23.407245","exception":false,"start_time":"2025-04-20T17:33:23.231955","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["üîë GOOGLE_API_KEY loaded: OK\n"]}],"source":["from kaggle_secrets import UserSecretsClient\n","\n","GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n","\n","print(\"üîë GOOGLE_API_KEY loaded:\", \"OK\" if GOOGLE_API_KEY else \"MISSING\")"]},{"cell_type":"markdown","id":"d3b2eb81","metadata":{"papermill":{"duration":0.004784,"end_time":"2025-04-20T17:33:23.417257","exception":false,"start_time":"2025-04-20T17:33:23.412473","status":"completed"},"tags":[]},"source":["## Embedding Model Selection Criteria\n","\n","For this project involving embedding PDF chunks (Summary of Benefits and Coverage documents) to build a Retrieval Augmented Generation (RAG) system for structured data extraction, the `models/text-embedding-004` model was selected over other available options (`models/embedding-001`, experimental models) for the following key reasons:\n","\n","1.  **Enhanced Retrieval Performance:** As a newer generation model, `text-embedding-004` generally demonstrates superior performance on retrieval benchmarks (like MTEB) compared to the older `models/embedding-001`. Better retrieval accuracy is crucial for RAG, as it ensures more relevant context is provided to the language model, leading to more accurate and complete answers.\n","2.  **Stability and General Availability:** While experimental models (`models/gemini-embedding-exp-*`) might offer potentially higher performance based on recent research, they lack the stability guarantees of a generally available (GA) model. `text-embedding-004` is a stable GA release, making it a more reliable choice for consistent development and potential future use compared to experimental versions that may change or have limitations.\n","3.  **Suitability for RAG Task:** This model is explicitly designed for semantic understanding and tasks like information retrieval and semantic search, which are fundamental to RAG. It also supports optional `task_type` parameters (e.g., `retrieval_document`, `retrieval_query`) that can further optimize embeddings specifically for the document chunking and querying stages of the RAG workflow.\n","\n","In conclusion, `models/text-embedding-004` provides a compelling combination of improved performance over older stable models and greater reliability than experimental versions, making it the most suitable choice for embedding the SBC documents in this RAG application."]},{"cell_type":"code","execution_count":3,"id":"03fbb168","metadata":{"execution":{"iopub.execute_input":"2025-04-20T17:33:23.428892Z","iopub.status.busy":"2025-04-20T17:33:23.428583Z","iopub.status.idle":"2025-04-20T17:33:23.880913Z","shell.execute_reply":"2025-04-20T17:33:23.880009Z"},"papermill":{"duration":0.460329,"end_time":"2025-04-20T17:33:23.882592","exception":false,"start_time":"2025-04-20T17:33:23.422263","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["models/embedding-001\n","models/text-embedding-004\n","models/gemini-embedding-exp-03-07\n","models/gemini-embedding-exp\n"]}],"source":["from google import genai\n","from google.genai import types\n","\n","client = genai.Client(api_key=GOOGLE_API_KEY)\n","\n","for model in client.models.list():\n","  if 'embedContent' in model.supported_actions:\n","    print(model.name)"]},{"cell_type":"markdown","id":"c4a3704b","metadata":{"papermill":{"duration":0.004874,"end_time":"2025-04-20T17:33:23.892673","exception":false,"start_time":"2025-04-20T17:33:23.887799","status":"completed"},"tags":[]},"source":["## Verify Simple Embedding & ChromaDB Write\n","\n","Before ingesting all PDFs, I run a quick ‚Äúhello world‚Äù embedding into a temporary ChromaDB (`./chroma_test_db`) to confirm:\n","\n","1. **Embedding & API connectivity** ‚Äì that `GoogleGenerativeAIEmbeddings` instantiates correctly and your key works.  \n","2. **Filesystem & Chroma write** ‚Äì that I have write permissions in `/kaggle/working/` and Chroma can persist data.\n","\n","Why this matters:\n","\n","- **Fail‚ÄëFast**: Catch core problems (readonly FS, bad key) in seconds, not minutes into a full ingestion.  \n","- **Environment Integrity**: Kaggle VMs occasionally have stale mounts or quota glitches‚Äîthis verifies the session is healthy.  \n","- **Cleanliness**: I isolate and then delete `./chroma_test_db`, so our real vector store stays pristine."]},{"cell_type":"code","execution_count":4,"id":"aa46f619","metadata":{"execution":{"iopub.execute_input":"2025-04-20T17:33:23.904654Z","iopub.status.busy":"2025-04-20T17:33:23.904308Z","iopub.status.idle":"2025-04-20T17:33:25.477796Z","shell.execute_reply":"2025-04-20T17:33:25.476703Z"},"papermill":{"duration":1.581734,"end_time":"2025-04-20T17:33:25.479608","exception":false,"start_time":"2025-04-20T17:33:23.897874","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["üîë GOOGLE_API_KEY loaded: OK\n","‚úÖ Embedding client initialized.\n","‚úÖ Embed dims: 768\n","‚úÖ Wrote single embedding to ChromaDB at './chroma_test_db', collection 'test_collection'\n","üóëÔ∏è Removed temporary ChromaDB directory: ./chroma_test_db\n"]}],"source":["import os\n","import shutil\n","\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain_community.vectorstores import Chroma\n","\n","def sanity_check_embedding_and_chroma(\n","    api_key: str,\n","    model_name: str = \"models/text-embedding-004\",\n","    test_text: str = \"hello world\",\n","    persist_dir: str = \"./chroma_test_db\",\n","    collection_name: str = \"test_collection\"\n","):\n","    \"\"\"\n","    Sanity‚Äëcheck embeddings and ChromaDB write access:\n","      1. Embed a single string.\n","      2. Write & read from a temporary Chroma vector store.\n","      3. Clean up the temporary directory.\n","    \"\"\"\n","    # 1) Verify API key & initialize embedder\n","    print(\"üîë GOOGLE_API_KEY loaded:\", \"OK\" if api_key else \"MISSING\")\n","    embedder = GoogleGenerativeAIEmbeddings(\n","        model=model_name,\n","        google_api_key=api_key,\n","        task_type=\"retrieval_query\"\n","    )\n","    print(\"‚úÖ Embedding client initialized.\")\n","\n","    # 2) Embed test string\n","    vector = embedder.embed_query(test_text)\n","    print(f\"‚úÖ Embed dims: {len(vector)}\")\n","\n","    # 3) Write to temporary Chroma\n","    os.makedirs(persist_dir, exist_ok=True)\n","    store = Chroma.from_texts(\n","        texts=[test_text],\n","        embedding=embedder,\n","        persist_directory=persist_dir,\n","        collection_name=collection_name\n","    )\n","    print(f\"‚úÖ Wrote single embedding to ChromaDB at '{persist_dir}', collection '{collection_name}'\")\n","\n","    # 4) Clean up\n","    try:\n","        shutil.rmtree(persist_dir)\n","        print(f\"üóëÔ∏è Removed temporary ChromaDB directory: {persist_dir}\")\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Could not remove '{persist_dir}': {e}\")\n","\n","if __name__ == \"__main__\":\n","    sanity_check_embedding_and_chroma(api_key=GOOGLE_API_KEY)\n"]},{"cell_type":"markdown","id":"305020ef","metadata":{"papermill":{"duration":0.004948,"end_time":"2025-04-20T17:33:25.490108","exception":false,"start_time":"2025-04-20T17:33:25.48516","status":"completed"},"tags":[]},"source":["## RAG Pipeline Overview\n","\n","Now I create a modular Retrieval‚ÄëAugmented Generation (RAG) workflow that transforms a collection of SBC PDFs into fully structured, per‚Äëplan JSON outputs by combining vector search with controlled LLM prompting.\n","\n","### 1. Document Ingestion & Indexing  \n","- **PDF Parsing & Chunking:** Load each policy PDF and split its text into overlapping segments (e.g. 500 characters with a 150‚Äëcharacter overlap) for manageable embedding and retrieval.  \n","- **Embedding into Vector Store:** Convert each text segment into a dense vector using Google‚Äôs text‚Äëembedding‚Äë004 model (with `task_type=\"retrieval_document\"`) and save both the vector and its metadata (source file, plan identifier) in a Chroma database.  \n","- **Robust Writes:** Batch writes with retry logic and small pauses to ensure resilience against transient failures or rate limits. I did this to mitigate `429` errors I was getting during testing.\n","\n","### 2. Multi‚ÄëQuery Context Retrieval  \n","- **Targeted Queries:** Define a collection of natural‚Äëlanguage questions‚Äîone per desired data field (e.g. effective dates, HSA availability, out‚Äëof‚Äëpocket limits, exclusions, links, etc.).  \n","- **Similarity Search:** For each question, retrieve the top¬†K (e.g.¬†5) most relevant text chunks from Chroma to give the LLM enough relevant information to answer accurately,\n","- **Context Consolidation:** Merge and deduplicate the results across all queries to build a single, comprehensive context pool that covers every aspect of each plan.\n","\n","### 3. Per‚ÄëPlan JSON Assembly  \n","- **Grouping by Plan:** Use stored metadata to partition the retrieved context so that each plan‚Äôs chunks are handled independently.  \n","- **Prompt Engineering:** For each plan, concatenate its text segments and feed them into an LLM prompt that includes:  \n","  1. Instructions positioning the model as an expert extractor  \n","  2. A clear JSON schema definition  \n","  3. A concise few‚Äëshot example  \n","  4. A strict ‚Äúuse only this context, output valid JSON‚Äù directive  \n","- **Structured Output & Parsing:** Invoke Gemini via `llm.invoke()`, strip any formatting fences, and parse the clean JSON into native objects‚Äîsubstituting `\"N/A\"` where fields are missing.\n","\n","### GenAI Capabilities Used\n","\n","- **Embeddings & Vector Store**: Converts text chunks into vectors optimized for document retrieval; stored in Chroma (Capabilities: Embeddings; Vector search / vector store).  \n","- **Document Understanding**: Uses PyMuPDF to reliably extract and split PDF content (Capability: Document understanding).  \n","- **Multi‚ÄëQuery RAG & Grounding**: Executes multiple targeted similarity searches and grounds the LLM‚Äôs output in retrieved context only (Capabilities: Retrieval-augmented generation; Grounding).  \n","- **Prompt Engineering for Structured Output**: Embeds a JSON schema and example in‚Äëprompt to guarantee valid, machine‚Äëparseable responses (Capability: Structured output / JSON mode).  \n","- **Few‚ÄëShot Prompting**: Supplies an in‚Äëprompt example to demonstrate desired format and content (Capability: Few‚Äëshot prompting).  \n"]},{"cell_type":"code","execution_count":5,"id":"dcec1c5c","metadata":{"execution":{"iopub.execute_input":"2025-04-20T17:33:25.50247Z","iopub.status.busy":"2025-04-20T17:33:25.502168Z","iopub.status.idle":"2025-04-20T17:33:33.352272Z","shell.execute_reply":"2025-04-20T17:33:33.350981Z"},"papermill":{"duration":7.858589,"end_time":"2025-04-20T17:33:33.353882","exception":false,"start_time":"2025-04-20T17:33:25.495293","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- PDF Ingestion Pipeline ---\n","üîë GOOGLE_API_KEY loaded: OK\n","üßπ Preparing workspace: /kaggle/working/chroma_db\n","   ‚úÖ Ensured directory exists.\n","‚öôÔ∏è Initializing text splitter (size=500, overlap=150)...\n","‚öôÔ∏è Initializing embedder (model=models/text-embedding-004)...\n","‚úÖ Embedder initialized.\n","‚öôÔ∏è Initializing Langchain Chroma store (dir=/kaggle/working/chroma_db)...\n","‚úÖ Chroma store initialized.\n","\n","üîç Searching for PDF files in: /kaggle/input/sbc-documents-small-set/\n","üéØ 3 PDFs found.\n","   ‚Ä¢ bluecross_anthem_hmo.pdf\n","   ‚Ä¢ bluecross_hsa.pdf\n","   ‚Ä¢ bluecross_ppo_250.pdf\n","\n","\n","üìÑ Processing bluecross_anthem_hmo.pdf...\n","‚úîÔ∏è Extracted 22,045 chars from bluecross_anthem_hmo.pdf\n","      ‚ÑπÔ∏è Using plan identifier: 'bluecross_anthem_hmo.pdf'\n","      Splitting into 61 chunks.\n","      Adding to store in 4 batches (size=20)...\n","         ‚ñ∂Ô∏è Batch 1/4 (20)... OK\n","         ‚ñ∂Ô∏è Batch 2/4 (20)... OK\n","         ‚ñ∂Ô∏è Batch 3/4 (20)... OK\n","         ‚ñ∂Ô∏è Batch 4/4 (1)... OK\n","      ‚úÖ Finished processing bluecross_anthem_hmo.pdf (61 chunks added).\n","\n","üìÑ Processing bluecross_hsa.pdf...\n","‚úîÔ∏è Extracted 23,167 chars from bluecross_hsa.pdf\n","      ‚ÑπÔ∏è Using plan identifier: 'bluecross_hsa.pdf'\n","      Splitting into 65 chunks.\n","      Adding to store in 4 batches (size=20)...\n","         ‚ñ∂Ô∏è Batch 1/4 (20)... OK\n","         ‚ñ∂Ô∏è Batch 2/4 (20)... OK\n","         ‚ñ∂Ô∏è Batch 3/4 (20)... OK\n","         ‚ñ∂Ô∏è Batch 4/4 (5)... OK\n","      ‚úÖ Finished processing bluecross_hsa.pdf (65 chunks added).\n","\n","üìÑ Processing bluecross_ppo_250.pdf...\n","‚úîÔ∏è Extracted 23,765 chars from bluecross_ppo_250.pdf\n","      ‚ÑπÔ∏è Using plan identifier: 'bluecross_ppo_250.pdf'\n","      Splitting into 66 chunks.\n","      Adding to store in 4 batches (size=20)...\n","         ‚ñ∂Ô∏è Batch 1/4 (20)... OK\n","         ‚ñ∂Ô∏è Batch 2/4 (20)... OK\n","         ‚ñ∂Ô∏è Batch 3/4 (20)... OK\n","         ‚ñ∂Ô∏è Batch 4/4 (6)... OK\n","      ‚úÖ Finished processing bluecross_ppo_250.pdf (66 chunks added).\n","\n","üìä Pipeline complete. Total chunks embedded: 192\n","\n","üèÅ --- Ingestion Finished ---\n"]}],"source":["import os\n","import glob\n","import fitz  # PyMuPDF\n","import time\n","import shutil\n","import traceback\n","from math import ceil\n","from typing import List, Dict, Any, Optional\n","\n","# Langchain and Chroma components\n","from langchain_chroma import Chroma\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n","from langchain.schema import Document # Optional: for type hinting if needed\n","\n","# Tenacity for retries\n","import tenacity # Import base tenacity for exception check\n","from tenacity import retry, stop_after_attempt, wait_exponential\n","\n","# ‚îÄ‚îÄ 1) Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","PDF_DIR: str = \"/kaggle/input/sbc-documents-small-set/\"\n","WORK_DIR: str = \"/kaggle/working/chroma_db\" # Persistence path for Chroma\n","EMBED_MODEL: str = \"models/text-embedding-004\"\n","CHUNK_SIZE: int = 500\n","CHUNK_OVERLAP: int = 150\n","BATCH_SIZE: int = 20  # Number of chunks to embed/add at once\n","BATCH_DELAY: float = 0.2 # Seconds to wait between batches\n","\n","# ‚îÄ‚îÄ 2) Initialization Functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","\n","def init_text_splitter(\n","    chunk_size: int = CHUNK_SIZE,\n","    chunk_overlap: int = CHUNK_OVERLAP\n",") -> TextSplitter:\n","    \"\"\"Initializes and returns a text splitter.\"\"\"\n","    print(f\"‚öôÔ∏è Initializing text splitter (size={chunk_size}, overlap={chunk_overlap})...\")\n","    # Using RecursiveCharacterTextSplitter as in the original code\n","    return RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size,\n","        chunk_overlap=chunk_overlap\n","    )\n","\n","def init_embedder(\n","    api_key: str,\n","    model_name: str = EMBED_MODEL\n",") -> Optional[GoogleGenerativeAIEmbeddings]:\n","    \"\"\"Initializes the Google Generative AI embedding client.\"\"\"\n","    print(f\"‚öôÔ∏è Initializing embedder (model={model_name})...\")\n","    if not api_key:\n","        print(\"‚ùå Error: GOOGLE_API_KEY not found or empty.\")\n","        return None\n","    try:\n","        embedder = GoogleGenerativeAIEmbeddings(\n","            model=model_name,\n","            google_api_key=api_key,\n","            task_type=\"retrieval_document\" # Specify task type for embeddings\n","        )\n","        print(\"‚úÖ Embedder initialized.\")\n","        return embedder\n","    except Exception as e:\n","        print(f\"‚ùå Failed to init embedder: {e}\")\n","        traceback.print_exc()\n","        return None\n","\n","def init_chroma_store(\n","    dir_path: str,\n","    embedder: GoogleGenerativeAIEmbeddings\n",") -> Optional[Chroma]:\n","    \"\"\"Initializes the Chroma vector store using Langchain wrapper.\"\"\"\n","    # This relies on the directory being cleaned beforehand by the calling function\n","    print(f\"‚öôÔ∏è Initializing Langchain Chroma store (dir={dir_path})...\")\n","    try:\n","        # Langchain Chroma wrapper handles client creation implicitly here\n","        store = Chroma(\n","            persist_directory=dir_path,\n","            embedding_function=embedder\n","        )\n","        print(\"‚úÖ Chroma store initialized.\")\n","        return store\n","    except Exception as e:\n","        print(f\"‚ùå Failed to init Chroma store: {e}\")\n","        traceback.print_exc()\n","        return None\n","\n","# ‚îÄ‚îÄ 3) PDF Processing Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","\n","def find_pdf_files(pdf_dir: str) -> List[str]:\n","    \"\"\"Finds and lists PDF files in the specified directory.\"\"\"\n","    print(f\"\\nüîç Searching for PDF files in: {pdf_dir}\")\n","    pdf_files = sorted(glob.glob(os.path.join(pdf_dir, \"*.pdf\")))\n","    print(f\"üéØ {len(pdf_files)} PDFs found.\")\n","    # Removed verbose listing for brevity, add back if needed\n","    if pdf_files:\n","        for p in pdf_files: print(\"   ‚Ä¢\", os.path.basename(p))\n","    print()\n","    return pdf_files\n","\n","def extract_text_from_pdf(pdf_path: str) -> str:\n","    \"\"\"Extracts full text content from a PDF file.\"\"\"\n","    pdf_fn = os.path.basename(pdf_path)\n","    try:\n","        doc = fitz.open(pdf_path)\n","        text = \"\".join(page.get_text() for page in doc)\n","        doc.close()\n","        print(f\"‚úîÔ∏è Extracted {len(text):,} chars from {pdf_fn}\")\n","        return text\n","    except Exception as e:\n","        print(f\"‚ùå Error reading {pdf_fn}: {e}\")\n","        # traceback.print_exc() # Optional: uncomment for more detail\n","        return \"\"\n","\n","def extract_plan_identifier(text: str, fallback_filename: str) -> str:\n","    \"\"\"Attempts to extract 'Plan Name:' from text, uses filename as fallback.\"\"\"\n","    identifier = fallback_filename # Default to filename\n","    try:\n","        lines = text.split('\\n', 20) # Search only the first few lines\n","        for line in lines:\n","            if line.strip().lower().startswith(\"plan name:\"):\n","                candidate = line.split(\":\", 1)[1].strip()\n","                if candidate:\n","                    identifier = candidate\n","                    break\n","    except Exception as e:\n","        print(f\"      ‚ö†Ô∏è Error extracting plan name: {e}\") # Log error but continue\n","    return identifier\n","\n","@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))\n","def _add_batch_with_retry(store: Chroma, texts: List[str], metadatas: List[Dict[str, Any]]):\n","    \"\"\"Internal retryable wrapper for adding texts to Chroma via Langchain.\"\"\"\n","    store.add_texts(texts=texts, metadatas=metadatas)\n","\n","\n","def process_pdf_file(\n","    pdf_path: str,\n","    splitter: TextSplitter,\n","    store: Chroma,\n","    batch_size: int = BATCH_SIZE,\n","    batch_delay: float = BATCH_DELAY\n",") -> int:\n","    \"\"\"Processes a single PDF: extracts, chunks, embeds via store.\"\"\"\n","    pdf_fn = os.path.basename(pdf_path)\n","    print(f\"\\nüìÑ Processing {pdf_fn}...\")\n","\n","    text = extract_text_from_pdf(pdf_path)\n","    if not text:\n","        print(\"      ‚ö†Ô∏è Skipping file (no text extracted).\")\n","        return 0\n","\n","    plan_id = extract_plan_identifier(text, pdf_fn)\n","    print(f\"      ‚ÑπÔ∏è Using plan identifier: '{plan_id}'\")\n","\n","    chunks = splitter.split_text(text)\n","    if not chunks:\n","        print(\"      ‚ö†Ô∏è No chunks generated after splitting.\")\n","        return 0\n","\n","    num_batches = ceil(len(chunks) / batch_size)\n","    print(f\"      Splitting into {len(chunks)} chunks.\")\n","    print(f\"      Adding to store in {num_batches} batches (size={batch_size})...\")\n","\n","    base_metadata = {\"source_file\": pdf_fn, \"plan_identifier\": plan_id}\n","    chunks_added_this_file = 0\n","\n","    for i in range(num_batches):\n","        start_idx = i * batch_size\n","        end_idx = (i + 1) * batch_size\n","        batch_texts = chunks[start_idx:end_idx]\n","        batch_metadatas = [\n","            {**base_metadata, \"chunk_index_in_doc\": start_idx + j}\n","            for j in range(len(batch_texts))\n","        ]\n","\n","        print(f\"         ‚ñ∂Ô∏è Batch {i+1}/{num_batches} ({len(batch_texts)})...\", end=\" \", flush=True)\n","        try:\n","            _add_batch_with_retry(store, batch_texts, batch_metadatas)\n","            chunks_added_this_file += len(batch_texts)\n","            print(\"OK\")\n","        except Exception as e:\n","            # Report the exception 'e' which might be RetryError wrapping the cause\n","            print(f\"FAILED! Batch error: {e}. Falling back...\")\n","            # Fallback: Try adding chunks individually\n","            for idx, single_chunk_text in enumerate(batch_texts):\n","                single_meta = batch_metadatas[idx]\n","                try:\n","                    # No retry on fallback add for simplicity\n","                    store.add_texts(texts=[single_chunk_text], metadatas=[single_meta])\n","                    chunks_added_this_file += 1\n","                except Exception as inner_e:\n","                    print(f\"              ‚ö†Ô∏è Chunk #{start_idx + idx} fallback error: {inner_e}\")\n","\n","        # Apply delay unless it's the last batch\n","        if i < num_batches - 1:\n","            time.sleep(batch_delay)\n","\n","    print(f\"      ‚úÖ Finished processing {pdf_fn} ({chunks_added_this_file} chunks added).\")\n","    return chunks_added_this_file\n","\n","\n","# ‚îÄ‚îÄ 4) Main Orchestration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","\n","def run_ingestion_pipeline(\n","    api_key: str,\n","    pdf_directory: str = PDF_DIR,\n","    work_directory: str = WORK_DIR,\n","):\n","    \"\"\"Runs the full PDF ingestion and embedding pipeline.\"\"\"\n","    print(\"--- PDF Ingestion Pipeline ---\")\n","    print(f\"üîë GOOGLE_API_KEY loaded: {'OK' if api_key else 'MISSING - Exiting.'}\")\n","    if not api_key: return\n","\n","    # 1. **Robust Directory Cleanup** (Essential for reruns)\n","    print(f\"üßπ Preparing workspace: {work_directory}\")\n","    if os.path.exists(work_directory):\n","        print(f\"   Attempting to delete existing directory...\")\n","        try:\n","            shutil.rmtree(work_directory) # No ignore_errors=True\n","            print(f\"   ‚úÖ Successfully deleted directory.\")\n","        except Exception as e:\n","            print(f\"   ‚ùå FAILED to delete directory: {e}. Exiting.\")\n","            traceback.print_exc()\n","            return # Stop if cleanup fails\n","    try:\n","        os.makedirs(work_directory, exist_ok=True)\n","        print(f\"   ‚úÖ Ensured directory exists.\")\n","    except Exception as e:\n","        print(f\"   ‚ùå FAILED to create directory: {e}. Exiting.\")\n","        traceback.print_exc()\n","        return\n","\n","    # 2. Initialize Resources\n","    splitter = init_text_splitter()\n","    embedder = init_embedder(api_key)\n","    if not embedder: return # Stop if embedder fails\n","\n","    # Initialize Chroma Store AFTER directory cleanup\n","    store = init_chroma_store(work_directory, embedder)\n","    if not store: return # Stop if store init fails\n","\n","    # 3. Find and Process PDFs\n","    pdf_files = find_pdf_files(pdf_directory)\n","    if not pdf_files:\n","        print(\"üèÅ No PDFs found. Finished.\")\n","        return\n","\n","    total_chunks_embedded = 0\n","    for pdf_path in pdf_files:\n","        chunks_added = process_pdf_file(\n","            pdf_path=pdf_path,\n","            splitter=splitter,\n","            store=store,\n","        )\n","        total_chunks_embedded += chunks_added\n","\n","    # 4. Summary\n","    print(f\"\\nüìä Pipeline complete. Total chunks embedded: {total_chunks_embedded}\")\n","    print(\"\\nüèÅ --- Ingestion Finished ---\")\n","\n","# ‚îÄ‚îÄ 5) Execution Trigger ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","\n","if __name__ == \"__main__\":\n","    run_ingestion_pipeline(api_key=GOOGLE_API_KEY)"]},{"cell_type":"code","execution_count":6,"id":"b4131be9","metadata":{"execution":{"iopub.execute_input":"2025-04-20T17:33:33.369298Z","iopub.status.busy":"2025-04-20T17:33:33.368969Z","iopub.status.idle":"2025-04-20T17:33:41.386117Z","shell.execute_reply":"2025-04-20T17:33:41.384575Z"},"papermill":{"duration":8.026734,"end_time":"2025-04-20T17:33:41.38763","exception":false,"start_time":"2025-04-20T17:33:33.360896","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["üîë GOOGLE_API_KEY loaded: OK\n","‚úÖ Embedding client initialized.\n","‚úÖ ChromaDB loaded (192 vectors).\n","‚úÖ LLM initialized (temp=0, top_p=1.0, top_k=1).\n","\n","üìÑ Retrieving top 5 chunks per query (9 queries)‚Ä¶\n","  -> 21 unique chunks retrieved.\n","\n","üìä 3 plans to process.\n","\n","--- Plan: bluecross_hsa.pdf (6 chunks) ---\n","\n","‚ú® Parsed JSON:\n","{\n","  \"carrierPlanName\": \"Anthem PPO HSA 3200/0\",\n","  \"startDate\": \"2024-07-01\",\n","  \"endDate\": \"2025-06-30\",\n","  \"coveragePeriod\": \"07/01/2024 - 06/30/2025\",\n","  \"issuingState\": \"CA\",\n","  \"summary\": \"Summary of Benefits and Coverage: What this Plan Covers & What You Pay for Covered Services. The SBC shows you how you and the plan would share the cost for covered health care services.\",\n","  \"links\": [\n","    {\n","      \"label\": \"network provider list\",\n","      \"url\": \"https://www.anthem.com/ca\"\n","    }\n","  ],\n","  \"hsaOffered\": true,\n","  \"out_of_pocket_max_values\": [\n","    {\n","      \"limit_type\": \"Individual/Person (In\\u2011Network)\",\n","      \"value\": \"$3,425\"\n","    },\n","    {\n","      \"limit_type\": \"Family (In\\u2011Network)\",\n","      \"value\": \"$6,850\"\n","    },\n","    {\n","      \"limit_type\": \"Individual/Person (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"$7,000\"\n","    },\n","    {\n","      \"limit_type\": \"Family (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"$14,000\"\n","    }\n","  ],\n","  \"out_of_pocket_exclusions\": \"Premiums, balance-billing charges, and health care this plan doesn't cover.\"\n","}\n","\n","--- Plan: bluecross_ppo_250.pdf (7 chunks) ---\n","\n","‚ú® Parsed JSON:\n","{\n","  \"carrierPlanName\": \"Anthem Classic PPO 250/20/10\",\n","  \"startDate\": \"2024-07-01\",\n","  \"endDate\": \"2025-06-30\",\n","  \"coveragePeriod\": \"07/01/2024 - 06/30/2025\",\n","  \"issuingState\": \"CA\",\n","  \"summary\": \"Summary of Benefits and Coverage: What this Plan Covers & What You Pay for Covered Services\",\n","  \"links\": [\n","    {\n","      \"label\": \"network providers\",\n","      \"url\": \"https://www.anthem.com/ca\"\n","    }\n","  ],\n","  \"hsaOffered\": null,\n","  \"out_of_pocket_max_values\": [\n","    {\n","      \"limit_type\": \"Individual/Person (In\\u2011Network)\",\n","      \"value\": \"$2,250\"\n","    },\n","    {\n","      \"limit_type\": \"Family (In\\u2011Network)\",\n","      \"value\": \"$4,500\"\n","    },\n","    {\n","      \"limit_type\": \"Individual/Person (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"$6,500\"\n","    },\n","    {\n","      \"limit_type\": \"Family (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"$13,000\"\n","    }\n","  ],\n","  \"out_of_pocket_exclusions\": \"Pre-Authorization Penalties, \\nPremiums, balance-billing \\ncharges, and health care this plan \\ndoesn't cover.\"\n","}\n","\n","--- Plan: bluecross_anthem_hmo.pdf (8 chunks) ---\n","\n","‚ú® Parsed JSON:\n","{\n","  \"carrierPlanName\": \"Anthem Classic HMO 15/30/250 admit /275 OP\",\n","  \"startDate\": \"2024-07-01\",\n","  \"endDate\": \"2025-06-30\",\n","  \"coveragePeriod\": \"07/01/2024 - 06/30/2025\",\n","  \"issuingState\": \"CA\",\n","  \"summary\": \"Summary of Benefits and Coverage: What this Plan Covers & What You Pay for Covered Services\",\n","  \"links\": [\n","    {\n","      \"label\": \"network providers\",\n","      \"url\": \"https://www.anthem.com/ca\"\n","    }\n","  ],\n","  \"hsaOffered\": null,\n","  \"out_of_pocket_max_values\": [\n","    {\n","      \"limit_type\": \"Individual/Person (In\\u2011Network)\",\n","      \"value\": \"$2,000\"\n","    },\n","    {\n","      \"limit_type\": \"Family (In\\u2011Network)\",\n","      \"value\": \"$4,000\"\n","    },\n","    {\n","      \"limit_type\": \"Individual/Person (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"N/A\"\n","    },\n","    {\n","      \"limit_type\": \"Family (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"N/A\"\n","    }\n","  ],\n","  \"out_of_pocket_exclusions\": \"Premiums, balance-billing charges, and health care this plan doesn't cover.\"\n","}\n","\n","üèÅ Done processing all plans.\n"]}],"source":["import os\n","import json\n","import traceback\n","from collections import defaultdict\n","\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI\n","from langchain_chroma import Chroma\n","\n","# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","WORK_DIR        = \"/kaggle/working/chroma_db\"\n","EMBED_MODEL     = \"models/text-embedding-004\"\n","LLM_MODEL_NAME  = \"gemini-2.0-flash\"\n","K_PER_QUERY     = 5\n","# Set these to be very deterministic to avoid hallucinations\n","TEMP, TOP_P, TOP_K = 0, 1.0, 1\n","\n","QUERIES_FOR_RETRIEVAL = {\n","    \"plan_id\":       \"What is the plan name, carrier name, plan code, and policy number?\",\n","    \"dates\":         \"What are the original effective date, start date, end date, and/or coverage date for the plan?\",\n","    \"location\":      \"What is the issuing state for this insurance plan?\",\n","    \"hsa_pcp\":       \"Does this plan offer an HSA (Health Savings Account)? Does it require PCP referrals?\",\n","    \"oop_max\":       \"What are the out‚Äëof‚Äëpocket maximums or limits for individual/family and in‚Äënetwork/out‚Äëof‚Äënetwork services?\",\n","    \"oop_exclusions\":\"What costs, premiums, or services are excluded from the out‚Äëof‚Äëpocket limit?\",\n","    \"links\":         \"Are there any website links or URLs mentioned for benefits or providers?\",\n","    \"summary_info\":  \"Provide general details about benefits, deductibles, copays, and overall cost sharing.\"\n","}\n","QUERIES_FOR_RETRIEVAL[\"coverage_period\"] = \\\n","  \"What is the coverage period of this plan? (e.g., 01/01/2024 - 12/31/2024)\"\n","QUERIES_FOR_RETRIEVAL[\"dates\"] = (\n","  \"What is the coverage period (start and end dates) for the plan?\"\n",")\n","\n","JSON_SCHEMA = \"\"\"{\n","  \"carrierPlanName\": \"\",\n","  \"startDate\": \"\",\n","  \"endDate\": \"\",\n","  \"coveragePeriod\": \"\",\n","  \"issuingState\": \"\",\n","  \"summary\": \"\",\n","  \"links\": [ { \"label\": \"\", \"url\": \"\" } ],\n","  \"hsaOffered\": null,\n","  \"out_of_pocket_max_values\": [\n","    { \"limit_type\": \"Individual/Person (In‚ÄëNetwork)\", \"value\": \"\" },\n","    { \"limit_type\": \"Family (In‚ÄëNetwork)\",             \"value\": \"\" },\n","    { \"limit_type\": \"Individual/Person (Out‚Äëof‚ÄëNetwork)\", \"value\": \"\" },\n","    { \"limit_type\": \"Family (Out‚Äëof‚ÄëNetwork)\",          \"value\": \"\" }\n","  ],\n","  \"out_of_pocket_exclusions\": \"\"\n","}\"\"\"\n","\n","EXAMPLE_JSON = \"\"\"{\n","  \"carrierPlanName\": \"Example Gold HMO\",\n","  \"startDate\": \"2023-01-01\",\n","  \"endDate\": \"2023-12-31\",\n","  \"issuingState\": \"CA\",\n","  \"summary\": \"An example summary highlighting key benefits and cost structure, approx 250 chars long.\",\n","  \"links\": [ { \"label\": \"Summary of Benefits\", \"url\": \"http://example.com/sbc\" } ],\n","  \"hsaOffered\": false,\n","  \"out_of_pocket_max_values\": [\n","    { \"limit_type\": \"Individual/Person (In‚ÄëNetwork)\", \"value\": \"$4000\" },\n","    { \"limit_type\": \"Family (In‚ÄëNetwork)\",             \"value\": \"$8000\" },\n","    { \"limit_type\": \"Individual/Person (Out‚Äëof‚ÄëNetwork)\", \"value\": \"$8000\" },\n","    { \"limit_type\": \"Family (Out‚Äëof‚ÄëNetwork)\",          \"value\": \"$16000\" }\n","  ],\n","  \"out_of_pocket_exclusions\": \"Premiums, non‚Äëcovered services.\"\n","}\"\"\"\n","\n","def init_embedder(api_key: str):\n","    try:\n","        emb = GoogleGenerativeAIEmbeddings(\n","            model=EMBED_MODEL,\n","            google_api_key=api_key,\n","            task_type=\"retrieval_query\"\n","        )\n","        print(\"‚úÖ Embedding client initialized.\")\n","        return emb\n","    except Exception as e:\n","        print(f\"‚ùå Failed to init embedder: {e}\")\n","        traceback.print_exc()\n","        return None\n","\n","def init_llm(\n","    api_key: str,\n","    temperature: float = 0.0,\n","    top_p: float = 1.0,\n","    top_k: int = 5\n","):\n","    \"\"\"\n","    Initialize the LLM client with sampling params.\n","     - temperature: 0.0 = deterministic\n","     - top_p: nucleus sampling (0.0‚Äì1.0)\n","     - top_k: top‚Äëk filtering\n","    \"\"\"\n","    try:\n","        llm = GoogleGenerativeAI(\n","            model=LLM_MODEL_NAME,\n","            google_api_key=api_key,\n","            temperature=temperature,\n","            top_p=top_p,\n","            top_k=top_k\n","        )\n","        print(f\"‚úÖ LLM initialized (temp={temperature}, top_p={top_p}, top_k={top_k}).\")\n","        return llm\n","    except Exception as e:\n","        print(f\"‚ùå Failed to init LLM: {e}\")\n","        traceback.print_exc()\n","        return None\n","\n","\n","def load_store(embedder):\n","    if not os.path.isdir(WORK_DIR) or not os.listdir(WORK_DIR):\n","        print(f\"‚ùå Error: ChromaDB '{WORK_DIR}' missing or empty.\")\n","        return None\n","    try:\n","        store = Chroma(persist_directory=WORK_DIR, embedding_function=embedder)\n","        count = store._collection.count()\n","        print(f\"‚úÖ ChromaDB loaded ({count} vectors).\")\n","        return store if count > 0 else None\n","    except Exception as e:\n","        print(f\"‚ùå Failed to load ChromaDB: {e}\")\n","        traceback.print_exc()\n","        return None\n","\n","def build_prompt(context: str) -> str:\n","    return (\n","        \"You are an expert health insurance information extraction assistant.\\n\"\n","        \"Extract ALL fields into the JSON object below using ONLY the provided context.\\n\\n\"\n","        \"JSON Schema:\\n```json\\n\" + JSON_SCHEMA + \"\\n```\\n\\n\"\n","        \"Example:\\n```json\\n\" + EXAMPLE_JSON + \"\\n```\\n\\n\"\n","        \"Context:\\n---START---\\n\" + context + \"\\n---END---\\n\\n\"\n","        \"Output ONLY the JSON object.\\n\"\n","    )\n","\n","def multi_query_retrieval(store):\n","    print(f\"\\nüìÑ Retrieving top {K_PER_QUERY} chunks per query ({len(QUERIES_FOR_RETRIEVAL)} queries)‚Ä¶\")\n","    all_docs, seen = [], set()\n","    for key, q in QUERIES_FOR_RETRIEVAL.items():\n","        try:\n","            for doc in store.similarity_search(q, k=K_PER_QUERY):\n","                if doc.page_content not in seen:\n","                    seen.add(doc.page_content)\n","                    all_docs.append(doc)\n","        except Exception as e:\n","            print(f\"‚ùå Retrieval error for '{key}': {e}\")\n","            traceback.print_exc()\n","    print(f\"  -> {len(all_docs)} unique chunks retrieved.\")\n","    return all_docs\n","\n","def group_by_plan(docs):\n","    grouped = defaultdict(list)\n","    for doc in docs:\n","        pid = doc.metadata.get(\"plan_identifier\", \"Unknown\")\n","        grouped[pid].append(doc)\n","    print(f\"\\nüìä {len(grouped)} plans to process.\")\n","    return grouped\n","\n","def process_plans(store, llm):\n","    docs = multi_query_retrieval(store)\n","    if not docs:\n","        print(\"\\n‚ö†Ô∏è No docs retrieved‚Äîskipping.\")\n","        return\n","\n","    plans = group_by_plan(docs)\n","    results = {}\n","\n","    for pid, chunks in plans.items():\n","        print(f\"\\n--- Plan: {pid} ({len(chunks)} chunks) ---\")\n","        context = \"\\n\\n---\\n\\n\".join(d.page_content for d in chunks)\n","        prompt = build_prompt(context)\n","\n","        try:\n","            resp = llm.invoke(prompt)\n","            clean = resp.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n","            data = json.loads(clean)\n","\n","            # ‚îÄ‚îÄ Replace null out‚Äëof‚Äënetwork values with \"N/A\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","            for entry in data.get(\"out_of_pocket_max_values\", []):\n","                if entry.get(\"value\") is None:\n","                    entry[\"value\"] = \"N/A\"\n","            \n","            print(\"\\n‚ú® Parsed JSON:\")\n","            print(json.dumps(data, indent=2))\n","            results[pid] = data\n","        except Exception as e:\n","            print(f\"‚ùå Error for plan '{pid}': {e}\")\n","            traceback.print_exc()\n","            results[pid] = {\"error\": str(e), \"raw\": resp}\n","\n","    print(\"\\nüèÅ Done processing all plans.\")\n","    return results\n","\n","\n","def main():\n","    print(\"üîë GOOGLE_API_KEY loaded:\", \"OK\" if GOOGLE_API_KEY else \"MISSING\")\n","\n","    embedder = init_embedder(GOOGLE_API_KEY)\n","    if not embedder:\n","        return\n","\n","    store = load_store(embedder)\n","    if not store:\n","        return\n","    llm = init_llm(GOOGLE_API_KEY, temperature=TEMP, top_p=TOP_P, top_k=TOP_K)\n","    if not llm:\n","        return\n","\n","    process_plans(store, llm)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","id":"f8210b0d","metadata":{"papermill":{"duration":0.007323,"end_time":"2025-04-20T17:33:41.402044","exception":false,"start_time":"2025-04-20T17:33:41.394721","status":"completed"},"tags":[]},"source":["## Chatbot Simulation for Open Enrollment Plan Comparison\n","\n","This section demonstrates how an employee might interact with a benefits chatbot during open enrollment: the system uses our RAG pipeline to extract structured JSON for each plan, then transforms that data into a clear, side‚Äëby‚Äëside comparison table highlighting out‚Äëof‚Äëpocket maxima and other key details so users don‚Äôt have to wade through multiple SBC PDFs.\n","\n","**Driving the Comparison**  \n","We first invoke `compare_plans`, which under the hood runs a multi‚Äëquery retrieval over all schema‚Äëtargeted questions, groups the resulting text chunks by plan, and prompts the LLM to produce clean JSON for each plan. Progress is surfaced with a `tqdm` bar, and any missing out‚Äëof‚Äënetwork values are replaced with ‚ÄúN/A‚Äù to ensure completeness.\n","\n","**Table Assembly & Styling**  \n","Next, I map each JSON field into human‚Äëreadable rows‚Äîplan name, HSA availability, in‚Äë and out‚Äëof‚Äënetwork out‚Äëof‚Äëpocket maxima, exclusions, and coverage dates‚Äîthen build a Pandas DataFrame with plans as columns. I strip file extensions, convert booleans to ‚ÄúYes‚Äù/‚ÄúNo,‚Äù normalize dates, and apply centered styling plus a bold caption to produce an employee‚Äëfriendly table right in the notebook.\n","\n","**Chatbot Session Simulation**  \n","Finally, a simple `main()` function ties it all together: I initialize the embedder, store, and LLM with slightly relaxed sampling parameters (temperature, top_p, top_k), echo a simulated user request, print a friendly ‚ÄúLet me look into that for you‚Ä¶‚Äù response, and render the styled comparison table‚Äîjust like a live chatbot session.\n"]},{"cell_type":"code","execution_count":7,"id":"1340115e","metadata":{"execution":{"iopub.execute_input":"2025-04-20T17:33:41.41754Z","iopub.status.busy":"2025-04-20T17:33:41.417212Z","iopub.status.idle":"2025-04-20T17:33:51.034719Z","shell.execute_reply":"2025-04-20T17:33:51.033766Z"},"papermill":{"duration":9.627373,"end_time":"2025-04-20T17:33:51.036321","exception":false,"start_time":"2025-04-20T17:33:41.408948","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["üîë GOOGLE_API_KEY loaded: OK\n","‚úÖ Embedding client initialized.\n","‚úÖ ChromaDB loaded (192 vectors).\n","‚úÖ LLM initialized (temp=0.2, top_p=0.9, top_k=8).\n","\n","üßë‚Äçüíº User: Please compare all medical plans my company offers and provide out‚Äëof‚Äëpocket‚Äëexpense summary in table format.\n","\n","ü§ñ ChatBot: Let me look into that for you...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"637d141d69044d5f8b243a6699984a9c","version_major":2,"version_minor":0},"text/plain":["Processing plans:   0%|          | 0/3 [00:00<?, ?plan/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_a2ec9 th {\n","  text-align: center;\n","}\n","#T_a2ec9 caption {\n","  font-weight: bold;\n","  font-size: 1.25rem;\n","  padding-bottom: 0.75em;\n","}\n","#T_a2ec9_row0_col0, #T_a2ec9_row0_col1, #T_a2ec9_row0_col2, #T_a2ec9_row1_col0, #T_a2ec9_row1_col1, #T_a2ec9_row1_col2, #T_a2ec9_row2_col0, #T_a2ec9_row2_col1, #T_a2ec9_row2_col2, #T_a2ec9_row3_col0, #T_a2ec9_row3_col1, #T_a2ec9_row3_col2, #T_a2ec9_row4_col0, #T_a2ec9_row4_col1, #T_a2ec9_row4_col2, #T_a2ec9_row5_col0, #T_a2ec9_row5_col1, #T_a2ec9_row5_col2, #T_a2ec9_row6_col0, #T_a2ec9_row6_col1, #T_a2ec9_row6_col2, #T_a2ec9_row7_col0, #T_a2ec9_row7_col1, #T_a2ec9_row7_col2, #T_a2ec9_row8_col0, #T_a2ec9_row8_col1, #T_a2ec9_row8_col2, #T_a2ec9_row9_col0, #T_a2ec9_row9_col1, #T_a2ec9_row9_col2 {\n","  text-align: center;\n","}\n","</style>\n","<table id=\"T_a2ec9\">\n","  <caption>Out‚Äëof‚ÄëPocket Expensese & Plan Details Comparison</caption>\n","  <thead>\n","    <tr>\n","      <th class=\"index_name level0\" >Plan</th>\n","      <th id=\"T_a2ec9_level0_col0\" class=\"col_heading level0 col0\" >bluecross_hsa</th>\n","      <th id=\"T_a2ec9_level0_col1\" class=\"col_heading level0 col1\" >bluecross_ppo_250</th>\n","      <th id=\"T_a2ec9_level0_col2\" class=\"col_heading level0 col2\" >bluecross_anthem_hmo</th>\n","    </tr>\n","    <tr>\n","      <th class=\"index_name level0\" ></th>\n","      <th class=\"blank col0\" >&nbsp;</th>\n","      <th class=\"blank col1\" >&nbsp;</th>\n","      <th class=\"blank col2\" >&nbsp;</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row0\" class=\"row_heading level0 row0\" >Plan Name</th>\n","      <td id=\"T_a2ec9_row0_col0\" class=\"data row0 col0\" >Anthem PPO HSA 3200/0</td>\n","      <td id=\"T_a2ec9_row0_col1\" class=\"data row0 col1\" >Anthem Classic PPO 250/20/10</td>\n","      <td id=\"T_a2ec9_row0_col2\" class=\"data row0 col2\" >Anthem Classic HMO 15/30/250 admit /275 OP</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row1\" class=\"row_heading level0 row1\" >HSA Offered</th>\n","      <td id=\"T_a2ec9_row1_col0\" class=\"data row1 col0\" >Yes</td>\n","      <td id=\"T_a2ec9_row1_col1\" class=\"data row1 col1\" >None</td>\n","      <td id=\"T_a2ec9_row1_col2\" class=\"data row1 col2\" >None</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row2\" class=\"row_heading level0 row2\" >OOP Max (Indv) (In‚ÄëNetwork)</th>\n","      <td id=\"T_a2ec9_row2_col0\" class=\"data row2 col0\" >$3,425</td>\n","      <td id=\"T_a2ec9_row2_col1\" class=\"data row2 col1\" >$2,250</td>\n","      <td id=\"T_a2ec9_row2_col2\" class=\"data row2 col2\" >$2,000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row3\" class=\"row_heading level0 row3\" >OOP Max (Fam) (In‚ÄëNetwork)</th>\n","      <td id=\"T_a2ec9_row3_col0\" class=\"data row3 col0\" >$6,850</td>\n","      <td id=\"T_a2ec9_row3_col1\" class=\"data row3 col1\" >$4,500</td>\n","      <td id=\"T_a2ec9_row3_col2\" class=\"data row3 col2\" >$4,000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row4\" class=\"row_heading level0 row4\" >OOP Max (Indv) (Out‚Äëof‚ÄëNetwork)</th>\n","      <td id=\"T_a2ec9_row4_col0\" class=\"data row4 col0\" >$7,000</td>\n","      <td id=\"T_a2ec9_row4_col1\" class=\"data row4 col1\" >$6,500</td>\n","      <td id=\"T_a2ec9_row4_col2\" class=\"data row4 col2\" >N/A</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row5\" class=\"row_heading level0 row5\" >OOP Max (Fam) (Out‚Äëof‚ÄëNetwork)</th>\n","      <td id=\"T_a2ec9_row5_col0\" class=\"data row5 col0\" >$14,000</td>\n","      <td id=\"T_a2ec9_row5_col1\" class=\"data row5 col1\" >$13,000</td>\n","      <td id=\"T_a2ec9_row5_col2\" class=\"data row5 col2\" >N/A</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row6\" class=\"row_heading level0 row6\" >OOP Exclusions</th>\n","      <td id=\"T_a2ec9_row6_col0\" class=\"data row6 col0\" >Premiums, balance-billing charges, and health care this plan doesn't cover.</td>\n","      <td id=\"T_a2ec9_row6_col1\" class=\"data row6 col1\" >Pre-Authorization Penalties, \n","Premiums, balance-billing \n","charges, and health care this plan \n","doesn't cover.</td>\n","      <td id=\"T_a2ec9_row6_col2\" class=\"data row6 col2\" >Premiums, balance-billing charges, and health care this plan doesn't cover.</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row7\" class=\"row_heading level0 row7\" >Issuing State</th>\n","      <td id=\"T_a2ec9_row7_col0\" class=\"data row7 col0\" >CA</td>\n","      <td id=\"T_a2ec9_row7_col1\" class=\"data row7 col1\" >CA</td>\n","      <td id=\"T_a2ec9_row7_col2\" class=\"data row7 col2\" >CA</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row8\" class=\"row_heading level0 row8\" >Start Date</th>\n","      <td id=\"T_a2ec9_row8_col0\" class=\"data row8 col0\" >07/01/2024</td>\n","      <td id=\"T_a2ec9_row8_col1\" class=\"data row8 col1\" >07/01/2024</td>\n","      <td id=\"T_a2ec9_row8_col2\" class=\"data row8 col2\" >07/01/2024</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a2ec9_level0_row9\" class=\"row_heading level0 row9\" >End Date</th>\n","      <td id=\"T_a2ec9_row9_col0\" class=\"data row9 col0\" >06/30/2025</td>\n","      <td id=\"T_a2ec9_row9_col1\" class=\"data row9 col1\" >06/30/2025</td>\n","      <td id=\"T_a2ec9_row9_col2\" class=\"data row9 col2\" >06/30/2025</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7935e78f5490>"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","import io\n","import json\n","import contextlib\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","from IPython.display import display\n","\n","def compare_plans(store, llm):\n","    \"\"\"\n","    Runs RAG + LLM per plan (with tqdm), then returns\n","    a cleaned & styled DataFrame (no .pdf suffixes, Yes/No,\n","    normalized dates) with a caption title.\n","    \"\"\"\n","    buf = io.StringIO()\n","    with contextlib.redirect_stdout(buf):\n","        docs  = multi_query_retrieval(store)\n","        plans = group_by_plan(docs)\n","\n","    results = {}\n","    for pid, chunks in tqdm(plans.items(), desc=\"Processing plans\", unit=\"plan\"):\n","        ctx    = \"\\n\\n---\\n\\n\".join(d.page_content for d in chunks)\n","        prompt = build_prompt(ctx)\n","        resp   = llm.invoke(prompt)\n","        clean  = resp.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n","        data   = json.loads(clean)\n","        for e in data.get(\"out_of_pocket_max_values\", []):\n","            if e.get(\"value\") is None:\n","                e[\"value\"] = \"N/A\"\n","        results[pid] = data\n","\n","    # Define rows\n","    sample      = next(iter(results.values()))\n","    limit_types = [v[\"limit_type\"] for v in sample[\"out_of_pocket_max_values\"]]\n","\n","    row_defs = [\n","        (\"Plan Name\",   lambda d: d.get(\"carrierPlanName\", \"\")),\n","        (\"HSA Offered\", lambda d: str(d.get(\"hsaOffered\", \"\"))),\n","    ]\n","    for lt in limit_types:\n","        label = lt.replace(\"Individual/Person\", \"OOP Max (Indv)\") \\\n","                  .replace(\"Family\", \"OOP Max (Fam)\")\n","        row_defs.append((\n","            label,\n","            lambda d, lt=lt: next(\n","                (x[\"value\"] for x in d[\"out_of_pocket_max_values\"] if x[\"limit_type\"]==lt),\n","                \"\"\n","            )\n","        ))\n","    row_defs += [\n","        (\"OOP Exclusions\", lambda d: d.get(\"out_of_pocket_exclusions\", \"\")),\n","        (\"Issuing State\",  lambda d: d.get(\"issuingState\", \"\")),\n","        (\"Start Date\",     lambda d: d.get(\"startDate\", \"\")),\n","        (\"End Date\",       lambda d: d.get(\"endDate\", \"\")),\n","    ]\n","\n","    # Build DataFrame\n","    plan_ids = list(results.keys())\n","    data     = {\n","        pid: [ extractor(results[pid]) for _, extractor in row_defs ]\n","        for pid in plan_ids\n","    }\n","    index = [ label for label, _ in row_defs ]\n","    df    = pd.DataFrame(data, index=index)\n","\n","    # 1) Strip .pdf from headers\n","    df.columns = [ os.path.splitext(c)[0] for c in df.columns ]\n","    df.columns.name = \"Plan\"\n","    df.index.name   = \"\"\n","\n","    # 2) Fallback Plan Name ‚Üí filename (no .pdf)\n","    df.loc[\"Plan Name\"] = [\n","        name if name else os.path.splitext(pid)[0]\n","        for name, pid in zip(df.loc[\"Plan Name\"], plan_ids)\n","    ]\n","\n","    # 3) True/False ‚Üí Yes/No\n","    df.replace({\"True\": \"Yes\", \"False\": \"No\"}, inplace=True)\n","\n","    # 4) Normalize dates to MM/DD/YYYY\n","    def fmt_date(x):\n","        try:\n","            dt = pd.to_datetime(x, errors=\"coerce\")\n","            return dt.strftime(\"%m/%d/%Y\") if not pd.isna(dt) else x\n","        except:\n","            return x\n","\n","    df.loc[\"Start Date\"] = df.loc[\"Start Date\"].apply(fmt_date)\n","    df.loc[\"End Date\"]   = df.loc[\"End Date\"].apply(fmt_date)\n","\n","    # 5) Style with caption\n","    styled = (\n","        df.style\n","          .set_caption(\"Out‚Äëof‚ÄëPocket Expensese & Plan Details Comparison\")\n","          .set_properties(**{\"text-align\": \"center\"})\n","          .set_table_styles([\n","              {\"selector\": \"th\",      \"props\": [(\"text-align\", \"center\")]},\n","              {\"selector\": \"caption\",\"props\": [\n","                  (\"font-weight\",    \"bold\"),\n","                  (\"font-size\", \"1.25rem\"),\n","                  (\"padding-bottom\",\"0.75em\")\n","              ]}\n","          ])\n","    )\n","    return styled\n","\n","def main():\n","    # Set these to be a little more relaxed for better NLP, yet still deterministic\n","    TEMP, TOP_P, TOP_K = 0.2, 0.9, 8\n","    \n","    # Verify Google API Key\n","    print(\"üîë GOOGLE_API_KEY loaded:\", \"OK\" if GOOGLE_API_KEY else \"MISSING\")\n","\n","    # Initialize\n","    embedder = init_embedder(GOOGLE_API_KEY)\n","    store    = load_store(embedder)\n","    llm      = init_llm(GOOGLE_API_KEY, temperature=TEMP, top_p=TOP_P, top_k=TOP_K)\n","    \n","    # Echo the prompt\n","    print(\"\\nüßë‚Äçüíº User: Please compare all medical plans my company offers and provide out‚Äëof‚Äëpocket‚Äëexpense summary in table format.\\n\")\n","\n","    # Compare and display\n","    print(f\"ü§ñ ChatBot: Let me look into that for you...\")\n","    styled_df = compare_plans(store, llm)\n","    print()\n","    display(styled_df)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","id":"0610ed1b","metadata":{"papermill":{"duration":0.006703,"end_time":"2025-04-20T17:33:51.050422","exception":false,"start_time":"2025-04-20T17:33:51.043719","status":"completed"},"tags":[]},"source":["## Final Thoughts\n","\n","This project highlighted the incredible potential of combining Generative AI with structured document extraction to achieve results that can be rendered nicely for human consumption or JSON for system integrations. While I illustrated how this can be nicely done for my use case with SBCs, you can see how this could be applied to other use cases. Legal contracts, financial reports, medical records‚Äîany field burdened by complex PDF-based documentation‚Äîcould immensely benefit from this AI-driven extraction workflow.\n","\n","As I continue to explore Google AIs capabilities for use cases at my own company, I encourage you to stay in touch.\n","\n","You can follow me through my [substack](https://daisyhealthcare.substack.com/) and on [LinkedIn](https://www.linkedin.com/in/lsacco/)."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":7166721,"sourceId":11456426,"sourceType":"datasetVersion"}],"dockerImageVersionId":31012,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":101.359411,"end_time":"2025-04-20T17:33:52.581261","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-20T17:32:11.22185","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"13317a69aefe466194f83dfedbc75860":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14934c99fa0b428aaf91fdd19f612b3b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2358a21d9cff41e9addee960f23707ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c4b00c944e30432783ab0e02125bdaf6","placeholder":"‚Äã","style":"IPY_MODEL_dd0d4896b117463eb01f9a68a9d6f066","tabbable":null,"tooltip":null,"value":"Processing‚Äáplans:‚Äá100%"}},"637d141d69044d5f8b243a6699984a9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2358a21d9cff41e9addee960f23707ec","IPY_MODEL_ec45b59d9c4d4b8c915bd01370b53069","IPY_MODEL_a94b1c8482454a65ba3971265a77b864"],"layout":"IPY_MODEL_ec0f11e5ea784c8595ce8bdfdf7046c4","tabbable":null,"tooltip":null}},"96e6f9e2e5ee4ff4b82c22bd810031bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a94b1c8482454a65ba3971265a77b864":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_14934c99fa0b428aaf91fdd19f612b3b","placeholder":"‚Äã","style":"IPY_MODEL_96e6f9e2e5ee4ff4b82c22bd810031bb","tabbable":null,"tooltip":null,"value":"‚Äá3/3‚Äá[00:05&lt;00:00,‚Äá‚Äá1.90s/plan]"}},"c4b00c944e30432783ab0e02125bdaf6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd0d4896b117463eb01f9a68a9d6f066":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"eb52445b8ab14278a5fc67839cabab28":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec0f11e5ea784c8595ce8bdfdf7046c4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec45b59d9c4d4b8c915bd01370b53069":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_eb52445b8ab14278a5fc67839cabab28","max":3.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_13317a69aefe466194f83dfedbc75860","tabbable":null,"tooltip":null,"value":3.0}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}