{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lousacco/sacco-gen-ai-intensive-course-capstone-2025q1?scriptVersionId=234714975\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"5551733f","metadata":{"papermill":{"duration":0.005264,"end_time":"2025-04-18T19:44:52.989241","exception":false,"start_time":"2025-04-18T19:44:52.983977","status":"completed"},"tags":[]},"source":["# Google Gen AI Intensive Course Capstone 2025Q1\n","\n","## Overview\n","\n","Employers and insurance providers spend significant time manually entering detailed benefits data into HRIS systems when setting up open enrollment. This data, usually captured in Summary of Benefits and Coverage (SBC) PDF documents, must be painstakingly re-entered, creating tedious and error-prone work. Additionally, employees often find it challenging to choose the most suitable medical, dental, or vision plan because comparing detailed benefits across multiple documents can be confusing. As a result, decisions frequently default to cost rather than overall value.\n","\n","Generative AI (GenAI) can address these pain points effectively by utilizing Retrieval-Augmented Generation (RAG). With a RAG-based system, SBC documents are ingested and embedded into a vector store at plan creation time. Employees can then interact through a user-friendly chat interface during open enrollment, asking specific questions about each plan. The system can dynamically generate comparison tables or personalized recommendations based on individual needs. This project illustrates how leveraging GenAI capabilities learned in this course can significantly simplify and improve benefits plan selection.\n","\n","## Goal\n","This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline leveraging Generative AI (Google Gemini) to extract structured, meaningful data from uploaded SBC PDFs, enabling easy and transparent comparison of insurance plans. Furthermore, this can serve as an example for any industry that relies heavily on PDF documents to share information with its clients. The objective is to illustrate what's possible with this use case that is easily extended to others.\n","\n","### Gen AI Capabilities Utilized\n","\n","-   **Embeddings & Vector Store:** Utilized `GoogleGenerativeAIEmbeddings` to create vector representations of SBC document text chunks, storing them in a `Chroma` vector database for efficient retrieval.\n","-   **Vector Search/Vector Store/Vector Database:** Employed `Chroma` as a vector database and performed similarity searches (`store.similarity_search`) to find document chunks relevant to specific queries (including a multi-query approach).\n","-   **Retrieval Augmented Generation (RAG) & Grounding:** Implemented a multi-query RAG pipeline that retrieves relevant document chunks (context) from the vector store and provides this context to an LLM, instructing it to base its answers *only* on this retrieved information, ensuring grounded responses.\n","-   **Document Understanding:** Processed PDF documents (SBCs) using `PyMuPDF`, extracted text content, and used the LLM to interpret this text to extract specific data points based on context.\n","-   **Structured Output (JSON mode/controlled generation):** Engineered detailed prompts, including schema definitions and strict formatting rules, to guide the LLM in generating responses formatted as valid JSON objects containing extracted SBC details.\n","-   **Few-Shot Prompting:** Included a concrete example of the desired JSON output structure within the LLM prompt to improve the accuracy and formatting of the generated response based on the provided context.\n","\n","Let's get started and for any issues don't hesitate to contact me directly through my Kaggle [profile](https://www.kaggle.com/lousacco)."]},{"cell_type":"markdown","id":"ad829381","metadata":{"papermill":{"duration":0.003581,"end_time":"2025-04-18T19:44:52.997052","exception":false,"start_time":"2025-04-18T19:44:52.993471","status":"completed"},"tags":[]},"source":["## Set-up SDK and Packages\n","\n","The first step is to set-up the SDK and include the required packages I'll use throughout this notebook. Note that there may be some dependency conflicts, perhaps due to some already installed packages that come with Kaggle. These are innocuous and do not affect the running of the rest of the project."]},{"cell_type":"code","execution_count":1,"id":"3d189977","metadata":{"execution":{"iopub.execute_input":"2025-04-18T19:44:53.006096Z","iopub.status.busy":"2025-04-18T19:44:53.005777Z","iopub.status.idle":"2025-04-18T19:45:56.766392Z","shell.execute_reply":"2025-04-18T19:45:56.765032Z"},"papermill":{"duration":63.767404,"end_time":"2025-04-18T19:45:56.768319","exception":false,"start_time":"2025-04-18T19:44:53.000915","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","jupyterlab-lsp 3.10.2 requires jupyterlab<4.0.0a0,>=3.1.0, which is not installed.\r\n","google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.24.2 which is incompatible.\r\n","google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.2 which is incompatible.\r\n","google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\r\n","bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m"]}],"source":["# Remove unused conflicting packages\n","!pip uninstall -qqy jupyterlab kfp 2>/dev/null\n","\n","# Upgrade pip first (Good Practice)\n","!pip install -q -U pip\n","\n","# Install/Upgrade all directly required packages together\n","# The -U flag will upgrade dependencies if needed by these packages\n","!pip install -q -U \\\n","    google-genai \\\n","    langchain-community \\\n","    PyMuPDF \\\n","    chromadb \\\n","    langchain-google-genai \\\n","    langchain-chroma \\\n","    tenacity"]},{"cell_type":"markdown","id":"3170e29f","metadata":{"papermill":{"duration":0.004818,"end_time":"2025-04-18T19:45:56.780031","exception":false,"start_time":"2025-04-18T19:45:56.775213","status":"completed"},"tags":[]},"source":["## Set up your API key\n","\n","To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n","\n","If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n","\n","To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."]},{"cell_type":"code","execution_count":2,"id":"a3304548","metadata":{"execution":{"iopub.execute_input":"2025-04-18T19:45:56.792539Z","iopub.status.busy":"2025-04-18T19:45:56.792135Z","iopub.status.idle":"2025-04-18T19:45:56.90059Z","shell.execute_reply":"2025-04-18T19:45:56.89915Z"},"papermill":{"duration":0.117358,"end_time":"2025-04-18T19:45:56.902421","exception":false,"start_time":"2025-04-18T19:45:56.785063","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ”‘ GOOGLE_API_KEY loaded: OK\n"]}],"source":["from kaggle_secrets import UserSecretsClient\n","\n","GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n","\n","print(\"ğŸ”‘ GOOGLE_API_KEY loaded:\", \"OK\" if GOOGLE_API_KEY else \"MISSING\")"]},{"cell_type":"markdown","id":"8c1308b7","metadata":{"papermill":{"duration":0.004925,"end_time":"2025-04-18T19:45:56.912593","exception":false,"start_time":"2025-04-18T19:45:56.907668","status":"completed"},"tags":[]},"source":["If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n","\n","![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)"]},{"cell_type":"markdown","id":"1fb6876c","metadata":{"papermill":{"duration":0.004925,"end_time":"2025-04-18T19:45:56.922504","exception":false,"start_time":"2025-04-18T19:45:56.917579","status":"completed"},"tags":[]},"source":["## Embedding Model Selection Criteria\n","\n","For this project involving embedding PDF chunks (Summary of Benefits and Coverage documents) to build a Retrieval Augmented Generation (RAG) system for structured data extraction, the `models/text-embedding-004` model was selected over other available options (`models/embedding-001`, experimental models) for the following key reasons:\n","\n","1.  **Enhanced Retrieval Performance:** As a newer generation model, `text-embedding-004` generally demonstrates superior performance on retrieval benchmarks (like MTEB) compared to the older `models/embedding-001`. Better retrieval accuracy is crucial for RAG, as it ensures more relevant context is provided to the language model, leading to more accurate and complete answers.\n","2.  **Stability and General Availability:** While experimental models (`models/gemini-embedding-exp-*`) might offer potentially higher performance based on recent research, they lack the stability guarantees of a generally available (GA) model. `text-embedding-004` is a stable GA release, making it a more reliable choice for consistent development and potential future use compared to experimental versions that may change or have limitations.\n","3.  **Suitability for RAG Task:** This model is explicitly designed for semantic understanding and tasks like information retrieval and semantic search, which are fundamental to RAG. It also supports optional `task_type` parameters (e.g., `retrieval_document`, `retrieval_query`) that can further optimize embeddings specifically for the document chunking and querying stages of the RAG workflow.\n","\n","In conclusion, `models/text-embedding-004` provides a compelling combination of improved performance over older stable models and greater reliability than experimental versions, making it the most suitable choice for embedding the SBC documents in this RAG application."]},{"cell_type":"code","execution_count":3,"id":"3fbc14cf","metadata":{"execution":{"iopub.execute_input":"2025-04-18T19:45:56.933795Z","iopub.status.busy":"2025-04-18T19:45:56.933482Z","iopub.status.idle":"2025-04-18T19:45:58.707479Z","shell.execute_reply":"2025-04-18T19:45:58.70627Z"},"papermill":{"duration":1.781407,"end_time":"2025-04-18T19:45:58.708931","exception":false,"start_time":"2025-04-18T19:45:56.927524","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["models/embedding-001\n","models/text-embedding-004\n","models/gemini-embedding-exp-03-07\n","models/gemini-embedding-exp\n"]}],"source":["from google import genai\n","from google.genai import types\n","\n","client = genai.Client(api_key=GOOGLE_API_KEY)\n","\n","for model in client.models.list():\n","  if 'embedContent' in model.supported_actions:\n","    print(model.name)"]},{"cell_type":"markdown","id":"604f1c97","metadata":{"papermill":{"duration":0.004919,"end_time":"2025-04-18T19:45:58.719352","exception":false,"start_time":"2025-04-18T19:45:58.714433","status":"completed"},"tags":[]},"source":["## Verify Simple Embedding & ChromaDB Write\n","\n","Before ingesting all PDFs, we run a quick â€œhello worldâ€ embedding into a temporary ChromaDB (`./chroma_test_db`) to confirm:\n","\n","1. **Embedding & API connectivity** â€“ that `GoogleGenerativeAIEmbeddings` instantiates correctly and your key works.  \n","2. **Filesystem & Chroma write** â€“ that we have write permissions in `/kaggle/working/` and Chroma can persist data.\n","\n","Why this matters:\n","\n","- **Failâ€‘Fast**: Catch core problems (readonly FS, bad key) in seconds, not minutes into a full ingestion.  \n","- **Environment Sanity**: Kaggle VMs occasionally have stale mounts or quota glitchesâ€”this verifies the session is healthy.  \n","- **Cleanliness**: We isolate and then delete `./chroma_test_db`, so our real vector store stays pristine.\n","\n","Once this passes, we confidently proceed to split, embed, and store all SBC PDFs into Chroma.\n"]},{"cell_type":"code","execution_count":4,"id":"2a8fbdfd","metadata":{"execution":{"iopub.execute_input":"2025-04-18T19:45:58.731703Z","iopub.status.busy":"2025-04-18T19:45:58.73077Z","iopub.status.idle":"2025-04-18T19:46:02.130899Z","shell.execute_reply":"2025-04-18T19:46:02.129969Z"},"papermill":{"duration":3.408293,"end_time":"2025-04-18T19:46:02.132701","exception":false,"start_time":"2025-04-18T19:45:58.724408","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ”‘ GOOGLE_API_KEY loaded: OK\n","âœ… Embedding client initialized.\n","âœ… Embed dims: 768\n","âœ… Wrote single embedding to ChromaDB at './chroma_test_db', collection 'test_collection'\n","ğŸ—‘ï¸ Removed temporary ChromaDB directory: ./chroma_test_db\n"]}],"source":["import os\n","import shutil\n","\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain_community.vectorstores import Chroma\n","\n","def sanity_check_embedding_and_chroma(\n","    api_key: str,\n","    model_name: str = \"models/text-embedding-004\",\n","    test_text: str = \"hello world\",\n","    persist_dir: str = \"./chroma_test_db\",\n","    collection_name: str = \"test_collection\"\n","):\n","    \"\"\"\n","    Sanityâ€‘check embeddings and ChromaDB write access:\n","      1. Embed a single string.\n","      2. Write & read from a temporary Chroma vector store.\n","      3. Clean up the temporary directory.\n","    \"\"\"\n","    # 1) Verify API key & initialize embedder\n","    print(\"ğŸ”‘ GOOGLE_API_KEY loaded:\", \"OK\" if api_key else \"MISSING\")\n","    embedder = GoogleGenerativeAIEmbeddings(\n","        model=model_name,\n","        google_api_key=api_key,\n","        task_type=\"retrieval_query\"\n","    )\n","    print(\"âœ… Embedding client initialized.\")\n","\n","    # 2) Embed test string\n","    vector = embedder.embed_query(test_text)\n","    print(f\"âœ… Embed dims: {len(vector)}\")\n","\n","    # 3) Write to temporary Chroma\n","    os.makedirs(persist_dir, exist_ok=True)\n","    store = Chroma.from_texts(\n","        texts=[test_text],\n","        embedding=embedder,\n","        persist_directory=persist_dir,\n","        collection_name=collection_name\n","    )\n","    print(f\"âœ… Wrote single embedding to ChromaDB at '{persist_dir}', collection '{collection_name}'\")\n","\n","    # 4) Clean up\n","    try:\n","        shutil.rmtree(persist_dir)\n","        print(f\"ğŸ—‘ï¸ Removed temporary ChromaDB directory: {persist_dir}\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Could not remove '{persist_dir}': {e}\")\n","\n","if __name__ == \"__main__\":\n","    sanity_check_embedding_and_chroma(api_key=GOOGLE_API_KEY)\n"]},{"cell_type":"markdown","id":"b8443b64","metadata":{"papermill":{"duration":0.005227,"end_time":"2025-04-18T19:46:02.143652","exception":false,"start_time":"2025-04-18T19:46:02.138425","status":"completed"},"tags":[]},"source":["## RAG Pipeline Overview\n","\n","A modular Retrievalâ€‘Augmented Generation (RAG) workflow that transforms a collection of SBC PDFs into fully structured, perâ€‘plan JSON outputs by combining vector search with controlled LLM prompting.\n","\n","### 1. Document Ingestion & Indexing  \n","- **PDF Parsing & Chunking:** Load each policy PDF and split its text into overlapping segments (e.g. 500 characters with a 150â€‘character overlap) for manageable embedding and retrieval.  \n","- **Embedding into Vector Store:** Convert each text segment into a dense vector using Googleâ€™s textâ€‘embeddingâ€‘004 model (with `task_type=\"retrieval_document\"`) and save both the vector and its metadata (source file, plan identifier) in a Chroma database.  \n","- **Robust Writes:** Batch writes with retry logic and small pauses to ensure resilience against transient failures or rate limits.\n","\n","### 2. Multiâ€‘Query Context Retrieval  \n","- **Targeted Queries:** Define a collection of naturalâ€‘language questionsâ€”one per desired data field (e.g. effective dates, HSA availability, outâ€‘ofâ€‘pocket limits, exclusions, links, etc.).  \n","- **Similarity Search:** For each question, retrieve the topÂ K (e.g.Â 5) most relevant text chunks from Chroma.  \n","- **Context Consolidation:** Merge and deduplicate the results across all queries to build a single, comprehensive context pool that covers every aspect of each plan.\n","\n","### 3. Perâ€‘Plan JSON Assembly  \n","- **Grouping by Plan:** Use stored metadata to partition the retrieved context so that each planâ€™s chunks are handled independently.  \n","- **Prompt Engineering:** For each plan, concatenate its text segments and feed them into an LLM prompt that includes:  \n","  1. Instructions positioning the model as an expert extractor  \n","  2. A clear JSON schema definition  \n","  3. A concise fewâ€‘shot example  \n","  4. A strict â€œuse only this context, output valid JSONâ€ directive  \n","- **Structured Output & Parsing:** Invoke Gemini via `llm.invoke()`, strip any formatting fences, and parse the clean JSON into native objectsâ€”substituting `\"N/A\"` where fields are missing.\n","\n","### Techniques & GenAI Capabilities\n","\n","- **Embeddings & Vector Store**: Converts text chunks into vectors optimized for document retrieval; stored in Chroma (Capabilities: Embeddings; Vector search / vector store).  \n","- **Document Understanding**: Uses PyMuPDF to reliably extract and split PDF content (Capability: Document understanding).  \n","- **Multiâ€‘Query RAG & Grounding**: Executes multiple targeted similarity searches and grounds the LLMâ€™s output in retrieved context only (Capabilities: Retrieval-augmented generation; Grounding).  \n","- **Prompt Engineering for Structured Output**: Embeds a JSON schema and example inâ€‘prompt to guarantee valid, machineâ€‘parseable responses (Capability: Structured output / JSON mode).  \n","- **Fewâ€‘Shot Prompting**: Supplies an inâ€‘prompt example to demonstrate desired format and content (Capability: Fewâ€‘shot prompting).  \n"]},{"cell_type":"code","execution_count":5,"id":"d4d87360","metadata":{"execution":{"iopub.execute_input":"2025-04-18T19:46:02.156795Z","iopub.status.busy":"2025-04-18T19:46:02.156147Z","iopub.status.idle":"2025-04-18T19:46:10.719894Z","shell.execute_reply":"2025-04-18T19:46:10.718997Z"},"papermill":{"duration":8.57254,"end_time":"2025-04-18T19:46:10.7215","exception":false,"start_time":"2025-04-18T19:46:02.14896","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ”‘ GOOGLE_API_KEY loaded: OK\n","âœ… Initialized Chroma store in: /kaggle/working/chroma_db\n","\n","ğŸ¯ 3 PDFs found:\n","    â€¢ bluecross_anthem_hmo.pdf\n","    â€¢ bluecross_hsa.pdf\n","    â€¢ bluecross_ppo_250.pdf\n","\n","\n","ğŸ“„ Processing bluecross_anthem_hmo.pdf...\n","âœ”ï¸ Extracted 22045 chars from bluecross_anthem_hmo.pdf\n","      â„¹ï¸ Using plan identifier: 'bluecross_anthem_hmo.pdf'\n","   Embedding 61 chunks in 4 batches...\n","      â–¶ï¸ Batch 1/4 (20 chunks)â€¦ OK\n","      â–¶ï¸ Batch 2/4 (20 chunks)â€¦ OK\n","      â–¶ï¸ Batch 3/4 (20 chunks)â€¦ OK\n","      â–¶ï¸ Batch 4/4 (1 chunks)â€¦ OK\n","   âœ… Finished processing bluecross_anthem_hmo.pdf (auto-persisted).\n","\n","ğŸ“„ Processing bluecross_hsa.pdf...\n","âœ”ï¸ Extracted 23167 chars from bluecross_hsa.pdf\n","      â„¹ï¸ Using plan identifier: 'bluecross_hsa.pdf'\n","   Embedding 65 chunks in 4 batches...\n","      â–¶ï¸ Batch 1/4 (20 chunks)â€¦ OK\n","      â–¶ï¸ Batch 2/4 (20 chunks)â€¦ OK\n","      â–¶ï¸ Batch 3/4 (20 chunks)â€¦ OK\n","      â–¶ï¸ Batch 4/4 (5 chunks)â€¦ OK\n","   âœ… Finished processing bluecross_hsa.pdf (auto-persisted).\n","\n","ğŸ“„ Processing bluecross_ppo_250.pdf...\n","âœ”ï¸ Extracted 23765 chars from bluecross_ppo_250.pdf\n","      â„¹ï¸ Using plan identifier: 'bluecross_ppo_250.pdf'\n","   Embedding 66 chunks in 4 batches...\n","      â–¶ï¸ Batch 1/4 (20 chunks)â€¦ OK\n","      â–¶ï¸ Batch 2/4 (20 chunks)â€¦ OK\n","      â–¶ï¸ Batch 3/4 (20 chunks)â€¦ OK\n","      â–¶ï¸ Batch 4/4 (6 chunks)â€¦ OK\n","   âœ… Finished processing bluecross_ppo_250.pdf (auto-persisted).\n","\n","ğŸ“Š Total chunks added across all files: 192\n","\n","ğŸ§ Inspecting final directory contents:\n","chroma_db/\n","  - chroma.sqlite3\n","  8052c02e-f490-4700-a3c1-659220346276/\n","    - header.bin\n","    - link_lists.bin\n","    - length.bin\n","    - data_level0.bin\n"]}],"source":["import os\n","import glob\n","import fitz\n","import time\n","import shutil\n","from math import ceil\n","\n","from langchain_chroma import Chroma\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from tenacity import retry, stop_after_attempt, wait_exponential\n","\n","# â”€â”€ 1) Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","PDF_DIR       = \"/kaggle/input/sbc-documents-small-set/\"\n","WORK_DIR      = \"/kaggle/working/chroma_db\"\n","EMBED_MODEL   = \"models/text-embedding-004\"\n","CHUNK_SIZE    = 500\n","CHUNK_OVERLAP = 150\n","BATCH_SIZE    = 20\n","BATCH_DELAY   = 0.2\n","\n","# â”€â”€ 2) Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def extract_text(path):\n","    \"\"\"Return full text of a PDF or empty on error.\"\"\"\n","    try:\n","        doc = fitz.open(path)\n","        txt = \"\".join(p.get_text() for p in doc)\n","        doc.close()\n","        print(f\"âœ”ï¸ Extracted {len(txt)} chars from {os.path.basename(path)}\")\n","        return txt\n","    except Exception as e:\n","        print(f\"âŒ Error reading {path}: {e}\")\n","        return \"\"\n","\n","def extract_plan_name_from_text(text, fallback):\n","    \"\"\"Pull a 'Plan Name:' line or fallback to filename.\"\"\"\n","    name = fallback\n","    try:\n","        for line in text.split(\"\\n\", 20):\n","            if line.strip().lower().startswith(\"plan name:\"):\n","                candidate = line.split(\":\", 1)[1].strip()\n","                if candidate:\n","                    name = candidate\n","                    break\n","    except Exception as e:\n","        print(f\"      âš ï¸ Error extracting plan name: {e}\")\n","    print(f\"      â„¹ï¸ Using plan identifier: '{name}'\")\n","    return name\n","\n","def main():\n","    # â”€â”€ 3) Load secret & workspace prep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    print(\"ğŸ”‘ GOOGLE_API_KEY loaded:\", \"OK\" if GOOGLE_API_KEY else \"MISSING\")\n","    \n","    # Re-create clean WORK_DIR\n","    if os.path.isdir(WORK_DIR):\n","        try:\n","            shutil.rmtree(WORK_DIR)\n","            print(f\"ğŸ—‘ï¸ Deleted existing ChromaDB directory: {WORK_DIR}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Error deleting {WORK_DIR}: {e}\")\n","    os.makedirs(WORK_DIR, exist_ok=True)\n","\n","    # â”€â”€ 4) Prepare splitter & embedder & store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=CHUNK_SIZE,\n","        chunk_overlap=CHUNK_OVERLAP\n","    )\n","    if not GOOGLE_API_KEY:\n","        raise ValueError(\"GOOGLE_API_KEY not found or empty. Cannot initialize embedder.\")\n","\n","    embedder = GoogleGenerativeAIEmbeddings(\n","        model=EMBED_MODEL,\n","        google_api_key=GOOGLE_API_KEY,\n","        task_type=\"retrieval_document\"\n","    )\n","    store = Chroma(persist_directory=WORK_DIR, embedding_function=embedder)\n","    print(f\"âœ… Initialized Chroma store in: {WORK_DIR}\")\n","\n","    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))\n","    def add_batch_with_retry(texts, metadatas):\n","        \"\"\"Retryable wrapper around store.add_texts().\"\"\"\n","        store.add_texts(texts=texts, metadatas=metadatas)\n","\n","    # â”€â”€ 5) Gather PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    pdfs = sorted(glob.glob(os.path.join(PDF_DIR, \"*.pdf\")))\n","    print(f\"\\nğŸ¯ {len(pdfs)} PDFs found:\")\n","    for p in pdfs:\n","        print(\"    â€¢\", os.path.basename(p))\n","    print()\n","\n","    # â”€â”€ 6) Process each PDF in batches â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    total_chunks_added = 0\n","\n","    for pdf_path in pdfs:\n","        pdf_fn = os.path.basename(pdf_path)\n","        print(f\"\\nğŸ“„ Processing {pdf_fn}...\")\n","        text = extract_text(pdf_path)\n","        if not text:\n","            print(\"   âš ï¸ Skipping file due to extraction error or empty content.\")\n","            continue\n","\n","        plan_id = extract_plan_name_from_text(text, pdf_fn)\n","        base_meta = {\n","            \"source_file\": pdf_fn,\n","            \"plan_identifier\": plan_id\n","        }\n","\n","        chunks = splitter.split_text(text)\n","        n_batch = ceil(len(chunks) / BATCH_SIZE)\n","        print(f\"   Embedding {len(chunks)} chunks in {n_batch} batches...\")\n","\n","        for i in range(n_batch):\n","            start, end = i * BATCH_SIZE, (i + 1) * BATCH_SIZE\n","            batch_texts = chunks[start:end]\n","            batch_metas = [\n","                {**base_meta, \"chunk_index_in_doc\": start + j}\n","                for j in range(len(batch_texts))\n","            ]\n","\n","            print(f\"      â–¶ï¸ Batch {i+1}/{n_batch} ({len(batch_texts)} chunks)â€¦\", end=\" \")\n","            try:\n","                add_batch_with_retry(batch_texts, batch_metas)\n","                total_chunks_added += len(batch_texts)\n","                print(\"OK\")\n","            except Exception as e:\n","                print(f\"FAILED! Error: {e}. Falling back to single-chunk mode...\")\n","                for idx, chunk in enumerate(batch_texts):\n","                    single_meta = batch_metas[idx]\n","                    try:\n","                        store.add_texts(texts=[chunk], metadatas=[single_meta])\n","                        total_chunks_added += 1\n","                    except Exception as inner:\n","                        print(f\"            âš ï¸ chunk#{start + idx} error: {inner}\")\n","            time.sleep(BATCH_DELAY)\n","\n","        print(f\"   âœ… Finished processing {pdf_fn} (auto-persisted).\")\n","\n","    # â”€â”€ 7) Final Summary and Inspection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    print(f\"\\nğŸ“Š Total chunks added across all files: {total_chunks_added}\")\n","\n","    print(\"\\nğŸ§ Inspecting final directory contents:\")\n","    for root, dirs, files in os.walk(WORK_DIR):\n","        indent = \"  \" * root.replace(WORK_DIR, \"\").count(os.sep)\n","        rel = os.path.relpath(root, WORK_DIR)\n","        print(f\"{indent}{os.path.basename(root)}/\")\n","        for f in files:\n","            print(f\"{indent}  - {f}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":6,"id":"078b0324","metadata":{"execution":{"iopub.execute_input":"2025-04-18T19:46:10.73645Z","iopub.status.busy":"2025-04-18T19:46:10.736073Z","iopub.status.idle":"2025-04-18T19:46:18.509462Z","shell.execute_reply":"2025-04-18T19:46:18.508099Z"},"papermill":{"duration":7.782872,"end_time":"2025-04-18T19:46:18.511127","exception":false,"start_time":"2025-04-18T19:46:10.728255","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- RAG Query: Multi-Query Retrieval + Per-Plan JSON Output ---\n","ğŸ”‘ GOOGLE_API_KEY loaded.\n","âœ… Embedding client initialized.\n","âœ… ChromaDB loaded (192 vectors).\n","âœ… LLM initialized (temp=0, top_p=1.0, top_k=1).\n","\n","ğŸ“„ Retrieving top 5 chunks per query (9 queries)â€¦\n","  -> 21 unique chunks retrieved.\n","\n","ğŸ“Š 3 plans to process.\n","\n","--- Plan: bluecross_hsa.pdf (6 chunks) ---\n","\n","âœ¨ Parsed JSON:\n","{\n","  \"carrierPlanName\": \"Anthem PPO HSA 3200/0\",\n","  \"startDate\": \"2024-07-01\",\n","  \"endDate\": \"2025-06-30\",\n","  \"coveragePeriod\": \"07/01/2024 - 06/30/2025\",\n","  \"issuingState\": \"CA\",\n","  \"summary\": \"Summary of Benefits and Coverage: What this Plan Covers & What You Pay for Covered Services. The SBC shows you how you and the plan would share the cost for covered health care services.\",\n","  \"links\": [\n","    {\n","      \"label\": \"network provider list\",\n","      \"url\": \"https://www.anthem.com/ca\"\n","    }\n","  ],\n","  \"hsaOffered\": true,\n","  \"out_of_pocket_max_values\": [\n","    {\n","      \"limit_type\": \"Individual/Person (In\\u2011Network)\",\n","      \"value\": \"$3,425\"\n","    },\n","    {\n","      \"limit_type\": \"Family (In\\u2011Network)\",\n","      \"value\": \"$6,850\"\n","    },\n","    {\n","      \"limit_type\": \"Individual/Person (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"$7,000\"\n","    },\n","    {\n","      \"limit_type\": \"Family (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"$14,000\"\n","    }\n","  ],\n","  \"out_of_pocket_exclusions\": \"Premiums, balance-billing charges, and health care this plan doesn't cover.\"\n","}\n","\n","--- Plan: bluecross_ppo_250.pdf (7 chunks) ---\n","\n","âœ¨ Parsed JSON:\n","{\n","  \"carrierPlanName\": \"Anthem Classic PPO 250/20/10\",\n","  \"startDate\": \"2024-07-01\",\n","  \"endDate\": \"2025-06-30\",\n","  \"coveragePeriod\": \"07/01/2024 - 06/30/2025\",\n","  \"issuingState\": \"CA\",\n","  \"summary\": \"Summary of Benefits and Coverage: What this Plan Covers & What You Pay for Covered Services\",\n","  \"links\": [\n","    {\n","      \"label\": \"network providers\",\n","      \"url\": \"https://www.anthem.com/ca\"\n","    }\n","  ],\n","  \"hsaOffered\": null,\n","  \"out_of_pocket_max_values\": [\n","    {\n","      \"limit_type\": \"Individual/Person (In\\u2011Network)\",\n","      \"value\": \"$2,250\"\n","    },\n","    {\n","      \"limit_type\": \"Family (In\\u2011Network)\",\n","      \"value\": \"$4,500\"\n","    },\n","    {\n","      \"limit_type\": \"Individual/Person (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"$6,500\"\n","    },\n","    {\n","      \"limit_type\": \"Family (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"$13,000\"\n","    }\n","  ],\n","  \"out_of_pocket_exclusions\": \"Pre-Authorization Penalties, \\nPremiums, balance-billing \\ncharges, and health care this plan \\ndoesn't cover.\"\n","}\n","\n","--- Plan: bluecross_anthem_hmo.pdf (8 chunks) ---\n","\n","âœ¨ Parsed JSON:\n","{\n","  \"carrierPlanName\": \"Anthem Classic HMO 15/30/250 admit /275 OP\",\n","  \"startDate\": \"2024-07-01\",\n","  \"endDate\": \"2025-06-30\",\n","  \"coveragePeriod\": \"07/01/2024 - 06/30/2025\",\n","  \"issuingState\": \"CA\",\n","  \"summary\": \"Summary of Benefits and Coverage: What this Plan Covers & What You Pay for Covered Services\",\n","  \"links\": [\n","    {\n","      \"label\": \"network providers\",\n","      \"url\": \"https://www.anthem.com/ca\"\n","    }\n","  ],\n","  \"hsaOffered\": null,\n","  \"out_of_pocket_max_values\": [\n","    {\n","      \"limit_type\": \"Individual/Person (In\\u2011Network)\",\n","      \"value\": \"$2,000\"\n","    },\n","    {\n","      \"limit_type\": \"Family (In\\u2011Network)\",\n","      \"value\": \"$4,000\"\n","    },\n","    {\n","      \"limit_type\": \"Individual/Person (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"N/A\"\n","    },\n","    {\n","      \"limit_type\": \"Family (Out\\u2011of\\u2011Network)\",\n","      \"value\": \"N/A\"\n","    }\n","  ],\n","  \"out_of_pocket_exclusions\": \"Premiums, balance-billing charges, and health care this plan doesn't cover.\"\n","}\n","\n","ğŸ Done processing all plans.\n"]}],"source":["import os\n","import json\n","import traceback\n","from collections import defaultdict\n","\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI\n","from langchain_chroma import Chroma\n","\n","# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","WORK_DIR        = \"/kaggle/working/chroma_db\"\n","EMBED_MODEL     = \"models/text-embedding-004\"\n","LLM_MODEL_NAME  = \"gemini-2.0-flash\"\n","K_PER_QUERY     = 5\n","# Set these to be very deterministic to avoid hallucinations\n","TEMP, TOP_P, TOP_K = 0, 1.0, 1\n","\n","QUERIES_FOR_RETRIEVAL = {\n","    \"plan_id\":       \"What is the plan name, carrier name, plan code, and policy number?\",\n","    \"dates\":         \"What are the original effective date, start date, end date, and/or coverage date for the plan?\",\n","    \"location\":      \"What is the issuing state for this insurance plan?\",\n","    \"hsa_pcp\":       \"Does this plan offer an HSA (Health Savings Account)? Does it require PCP referrals?\",\n","    \"oop_max\":       \"What are the outâ€‘ofâ€‘pocket maximums or limits for individual/family and inâ€‘network/outâ€‘ofâ€‘network services?\",\n","    \"oop_exclusions\":\"What costs, premiums, or services are excluded from the outâ€‘ofâ€‘pocket limit?\",\n","    \"links\":         \"Are there any website links or URLs mentioned for benefits or providers?\",\n","    \"summary_info\":  \"Provide general details about benefits, deductibles, copays, and overall cost sharing.\"\n","}\n","QUERIES_FOR_RETRIEVAL[\"coverage_period\"] = \\\n","  \"What is the coverage period of this plan? (e.g., 01/01/2024 - 12/31/2024)\"\n","QUERIES_FOR_RETRIEVAL[\"dates\"] = (\n","  \"What is the coverage period (start and end dates) for the plan?\"\n",")\n","\n","JSON_SCHEMA = \"\"\"{\n","  \"carrierPlanName\": \"\",\n","  \"startDate\": \"\",\n","  \"endDate\": \"\",\n","  \"coveragePeriod\": \"\",\n","  \"issuingState\": \"\",\n","  \"summary\": \"\",\n","  \"links\": [ { \"label\": \"\", \"url\": \"\" } ],\n","  \"hsaOffered\": null,\n","  \"out_of_pocket_max_values\": [\n","    { \"limit_type\": \"Individual/Person (Inâ€‘Network)\", \"value\": \"\" },\n","    { \"limit_type\": \"Family (Inâ€‘Network)\",             \"value\": \"\" },\n","    { \"limit_type\": \"Individual/Person (Outâ€‘ofâ€‘Network)\", \"value\": \"\" },\n","    { \"limit_type\": \"Family (Outâ€‘ofâ€‘Network)\",          \"value\": \"\" }\n","  ],\n","  \"out_of_pocket_exclusions\": \"\"\n","}\"\"\"\n","\n","EXAMPLE_JSON = \"\"\"{\n","  \"carrierPlanName\": \"Example Gold HMO\",\n","  \"startDate\": \"2023-01-01\",\n","  \"endDate\": \"2023-12-31\",\n","  \"issuingState\": \"CA\",\n","  \"summary\": \"An example summary highlighting key benefits and cost structure, approx 250 chars long.\",\n","  \"links\": [ { \"label\": \"Summary of Benefits\", \"url\": \"http://example.com/sbc\" } ],\n","  \"hsaOffered\": false,\n","  \"out_of_pocket_max_values\": [\n","    { \"limit_type\": \"Individual/Person (Inâ€‘Network)\", \"value\": \"$4000\" },\n","    { \"limit_type\": \"Family (Inâ€‘Network)\",             \"value\": \"$8000\" },\n","    { \"limit_type\": \"Individual/Person (Outâ€‘ofâ€‘Network)\", \"value\": \"$8000\" },\n","    { \"limit_type\": \"Family (Outâ€‘ofâ€‘Network)\",          \"value\": \"$16000\" }\n","  ],\n","  \"out_of_pocket_exclusions\": \"Premiums, nonâ€‘covered services.\"\n","}\"\"\"\n","\n","def load_api_key():\n","    key = GOOGLE_API_KEY\n","    if not key:\n","        print(\"âš ï¸ Warning: GOOGLE_API_KEY not found.\")\n","    else:\n","        print(\"ğŸ”‘ GOOGLE_API_KEY loaded.\")\n","    return key\n","\n","def init_embedder(api_key: str):\n","    try:\n","        emb = GoogleGenerativeAIEmbeddings(\n","            model=EMBED_MODEL,\n","            google_api_key=api_key,\n","            task_type=\"retrieval_query\"\n","        )\n","        print(\"âœ… Embedding client initialized.\")\n","        return emb\n","    except Exception as e:\n","        print(f\"âŒ Failed to init embedder: {e}\")\n","        traceback.print_exc()\n","        return None\n","\n","def init_llm(\n","    api_key: str,\n","    temperature: float = 0.0,\n","    top_p: float = 1.0,\n","    top_k: int = 5\n","):\n","    \"\"\"\n","    Initialize the LLM client with sampling params.\n","     - temperature: 0.0 = deterministic\n","     - top_p: nucleus sampling (0.0â€“1.0)\n","     - top_k: topâ€‘k filtering\n","    \"\"\"\n","    try:\n","        llm = GoogleGenerativeAI(\n","            model=LLM_MODEL_NAME,\n","            google_api_key=api_key,\n","            temperature=temperature,\n","            top_p=top_p,\n","            top_k=top_k\n","        )\n","        print(f\"âœ… LLM initialized (temp={temperature}, top_p={top_p}, top_k={top_k}).\")\n","        return llm\n","    except Exception as e:\n","        print(f\"âŒ Failed to init LLM: {e}\")\n","        traceback.print_exc()\n","        return None\n","\n","\n","def load_store(embedder):\n","    if not os.path.isdir(WORK_DIR) or not os.listdir(WORK_DIR):\n","        print(f\"âŒ Error: ChromaDB '{WORK_DIR}' missing or empty.\")\n","        return None\n","    try:\n","        store = Chroma(persist_directory=WORK_DIR, embedding_function=embedder)\n","        count = store._collection.count()\n","        print(f\"âœ… ChromaDB loaded ({count} vectors).\")\n","        return store if count > 0 else None\n","    except Exception as e:\n","        print(f\"âŒ Failed to load ChromaDB: {e}\")\n","        traceback.print_exc()\n","        return None\n","\n","def build_prompt(context: str) -> str:\n","    return (\n","        \"You are an expert health insurance information extraction assistant.\\n\"\n","        \"Extract ALL fields into the JSON object below using ONLY the provided context.\\n\\n\"\n","        \"JSON Schema:\\n```json\\n\" + JSON_SCHEMA + \"\\n```\\n\\n\"\n","        \"Example:\\n```json\\n\" + EXAMPLE_JSON + \"\\n```\\n\\n\"\n","        \"Context:\\n---START---\\n\" + context + \"\\n---END---\\n\\n\"\n","        \"Output ONLY the JSON object.\\n\"\n","    )\n","\n","def multi_query_retrieval(store):\n","    print(f\"\\nğŸ“„ Retrieving top {K_PER_QUERY} chunks per query ({len(QUERIES_FOR_RETRIEVAL)} queries)â€¦\")\n","    all_docs, seen = [], set()\n","    for key, q in QUERIES_FOR_RETRIEVAL.items():\n","        try:\n","            for doc in store.similarity_search(q, k=K_PER_QUERY):\n","                if doc.page_content not in seen:\n","                    seen.add(doc.page_content)\n","                    all_docs.append(doc)\n","        except Exception as e:\n","            print(f\"âŒ Retrieval error for '{key}': {e}\")\n","            traceback.print_exc()\n","    print(f\"  -> {len(all_docs)} unique chunks retrieved.\")\n","    return all_docs\n","\n","def group_by_plan(docs):\n","    grouped = defaultdict(list)\n","    for doc in docs:\n","        pid = doc.metadata.get(\"plan_identifier\", \"Unknown\")\n","        grouped[pid].append(doc)\n","    print(f\"\\nğŸ“Š {len(grouped)} plans to process.\")\n","    return grouped\n","\n","def process_plans(store, llm):\n","    docs = multi_query_retrieval(store)\n","    if not docs:\n","        print(\"\\nâš ï¸ No docs retrievedâ€”skipping.\")\n","        return\n","\n","    plans = group_by_plan(docs)\n","    results = {}\n","\n","    for pid, chunks in plans.items():\n","        print(f\"\\n--- Plan: {pid} ({len(chunks)} chunks) ---\")\n","        context = \"\\n\\n---\\n\\n\".join(d.page_content for d in chunks)\n","        prompt = build_prompt(context)\n","\n","        try:\n","            resp = llm.invoke(prompt)\n","            clean = resp.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n","            data = json.loads(clean)\n","\n","            # â”€â”€ Replace null outâ€‘ofâ€‘network values with \"N/A\" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","            for entry in data.get(\"out_of_pocket_max_values\", []):\n","                if entry.get(\"value\") is None:\n","                    entry[\"value\"] = \"N/A\"\n","            \n","            print(\"\\nâœ¨ Parsed JSON:\")\n","            print(json.dumps(data, indent=2))\n","            results[pid] = data\n","        except Exception as e:\n","            print(f\"âŒ Error for plan '{pid}': {e}\")\n","            traceback.print_exc()\n","            results[pid] = {\"error\": str(e), \"raw\": resp}\n","\n","    print(\"\\nğŸ Done processing all plans.\")\n","    return results\n","\n","\n","def main():\n","    print(\"--- RAG Query: Multi-Query Retrieval + Per-Plan JSON Output ---\")\n","    api_key = load_api_key()\n","    if not api_key:\n","        return\n","\n","    embedder = init_embedder(api_key)\n","    if not embedder:\n","        return\n","\n","    store = load_store(embedder)\n","    if not store:\n","        return\n","    llm = init_llm(api_key, temperature=TEMP, top_p=TOP_P, top_k=TOP_K)\n","    if not llm:\n","        return\n","\n","    process_plans(store, llm)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","id":"99f5a55f","metadata":{"papermill":{"duration":0.006394,"end_time":"2025-04-18T19:46:18.524604","exception":false,"start_time":"2025-04-18T19:46:18.51821","status":"completed"},"tags":[]},"source":["## Chatbot Simulation for Open Enrollment Plan Comparison\n","\n","This section demonstrates how an employee might interact with a benefits chatbot during open enrollment: the system uses our RAG pipeline to extract structured JSON for each plan, then transforms that data into a clear, sideâ€‘byâ€‘side comparison table highlighting outâ€‘ofâ€‘pocket maxima and other key details so users donâ€™t have to wade through multiple SBC PDFs.\n","\n","**Driving the Comparison**  \n","We first invoke `compare_plans`, which under the hood runs a multiâ€‘query retrieval over all schemaâ€‘targeted questions, groups the resulting text chunks by plan, and prompts the LLM to produce clean JSON for each plan. Progress is surfaced with a `tqdm` bar, and any missing outâ€‘ofâ€‘network values are replaced with â€œN/Aâ€ to ensure completeness.\n","\n","**Table Assembly & Styling**  \n","Next, we map each JSON field into humanâ€‘readable rowsâ€”plan name, HSA availability, inâ€‘ and outâ€‘ofâ€‘network outâ€‘ofâ€‘pocket maxima, exclusions, and coverage datesâ€”then build a Pandas DataFrame with plans as columns. We strip file extensions, convert booleans to â€œYesâ€/â€œNo,â€ normalize dates, and apply centered styling plus a bold caption to produce an employeeâ€‘friendly table right in the notebook.\n","\n","**Chatbot Session Simulation**  \n","Finally, a simple `main()` function ties it all together: we initialize the embedder, store, and LLM with slightly relaxed sampling parameters (temperature, top_p, top_k), echo a simulated user request, print a friendly â€œLet me look into that for youâ€¦â€ response, and render the styled comparison tableâ€”just like a live chatbot session.\n"]},{"cell_type":"code","execution_count":7,"id":"51585539","metadata":{"execution":{"iopub.execute_input":"2025-04-18T19:46:18.539904Z","iopub.status.busy":"2025-04-18T19:46:18.539592Z","iopub.status.idle":"2025-04-18T19:46:28.565962Z","shell.execute_reply":"2025-04-18T19:46:28.564681Z"},"papermill":{"duration":10.036156,"end_time":"2025-04-18T19:46:28.567748","exception":false,"start_time":"2025-04-18T19:46:18.531592","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ”‘ GOOGLE_API_KEY loaded.\n","âœ… Embedding client initialized.\n","âœ… ChromaDB loaded (192 vectors).\n","âœ… LLM initialized (temp=0.2, top_p=0.9, top_k=8).\n","\n","ğŸ§‘â€ğŸ’¼ User: Please compare all three plans and provide outâ€‘ofâ€‘pocketâ€‘expense summary in table format.\n","\n","ğŸ¤– ChatBot: Let me look into that for you...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0db84d426a7143ddb5d7416ba671d62d","version_major":2,"version_minor":0},"text/plain":["Processing plans:   0%|          | 0/3 [00:00<?, ?plan/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_abdc0 th {\n","  text-align: center;\n","}\n","#T_abdc0 caption {\n","  font-weight: bold;\n","  font-size: 1.25rem;\n","  padding-bottom: 0.75em;\n","}\n","#T_abdc0_row0_col0, #T_abdc0_row0_col1, #T_abdc0_row0_col2, #T_abdc0_row1_col0, #T_abdc0_row1_col1, #T_abdc0_row1_col2, #T_abdc0_row2_col0, #T_abdc0_row2_col1, #T_abdc0_row2_col2, #T_abdc0_row3_col0, #T_abdc0_row3_col1, #T_abdc0_row3_col2, #T_abdc0_row4_col0, #T_abdc0_row4_col1, #T_abdc0_row4_col2, #T_abdc0_row5_col0, #T_abdc0_row5_col1, #T_abdc0_row5_col2, #T_abdc0_row6_col0, #T_abdc0_row6_col1, #T_abdc0_row6_col2, #T_abdc0_row7_col0, #T_abdc0_row7_col1, #T_abdc0_row7_col2, #T_abdc0_row8_col0, #T_abdc0_row8_col1, #T_abdc0_row8_col2, #T_abdc0_row9_col0, #T_abdc0_row9_col1, #T_abdc0_row9_col2 {\n","  text-align: center;\n","}\n","</style>\n","<table id=\"T_abdc0\">\n","  <caption>Outâ€‘ofâ€‘Pocket Expensese & Plan Details Comparison</caption>\n","  <thead>\n","    <tr>\n","      <th class=\"index_name level0\" >Plan</th>\n","      <th id=\"T_abdc0_level0_col0\" class=\"col_heading level0 col0\" >bluecross_hsa</th>\n","      <th id=\"T_abdc0_level0_col1\" class=\"col_heading level0 col1\" >bluecross_ppo_250</th>\n","      <th id=\"T_abdc0_level0_col2\" class=\"col_heading level0 col2\" >bluecross_anthem_hmo</th>\n","    </tr>\n","    <tr>\n","      <th class=\"index_name level0\" ></th>\n","      <th class=\"blank col0\" >&nbsp;</th>\n","      <th class=\"blank col1\" >&nbsp;</th>\n","      <th class=\"blank col2\" >&nbsp;</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row0\" class=\"row_heading level0 row0\" >Plan Name</th>\n","      <td id=\"T_abdc0_row0_col0\" class=\"data row0 col0\" >Anthem PPO HSA 3200/0</td>\n","      <td id=\"T_abdc0_row0_col1\" class=\"data row0 col1\" >Anthem Classic PPO 250/20/10</td>\n","      <td id=\"T_abdc0_row0_col2\" class=\"data row0 col2\" >Anthem Classic HMO 15/30/250 admit /275 OP/1FHW/07-24</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row1\" class=\"row_heading level0 row1\" >HSA Offered</th>\n","      <td id=\"T_abdc0_row1_col0\" class=\"data row1 col0\" >Yes</td>\n","      <td id=\"T_abdc0_row1_col1\" class=\"data row1 col1\" >None</td>\n","      <td id=\"T_abdc0_row1_col2\" class=\"data row1 col2\" >None</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row2\" class=\"row_heading level0 row2\" >OOP Max (Indv) (Inâ€‘Network)</th>\n","      <td id=\"T_abdc0_row2_col0\" class=\"data row2 col0\" >$3,425</td>\n","      <td id=\"T_abdc0_row2_col1\" class=\"data row2 col1\" >$2,250</td>\n","      <td id=\"T_abdc0_row2_col2\" class=\"data row2 col2\" >$2,000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row3\" class=\"row_heading level0 row3\" >OOP Max (Fam) (Inâ€‘Network)</th>\n","      <td id=\"T_abdc0_row3_col0\" class=\"data row3 col0\" >$6,850</td>\n","      <td id=\"T_abdc0_row3_col1\" class=\"data row3 col1\" >$4,500</td>\n","      <td id=\"T_abdc0_row3_col2\" class=\"data row3 col2\" >$4,000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row4\" class=\"row_heading level0 row4\" >OOP Max (Indv) (Outâ€‘ofâ€‘Network)</th>\n","      <td id=\"T_abdc0_row4_col0\" class=\"data row4 col0\" >$7,000</td>\n","      <td id=\"T_abdc0_row4_col1\" class=\"data row4 col1\" >$6,500</td>\n","      <td id=\"T_abdc0_row4_col2\" class=\"data row4 col2\" >N/A</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row5\" class=\"row_heading level0 row5\" >OOP Max (Fam) (Outâ€‘ofâ€‘Network)</th>\n","      <td id=\"T_abdc0_row5_col0\" class=\"data row5 col0\" >$14,000</td>\n","      <td id=\"T_abdc0_row5_col1\" class=\"data row5 col1\" >$13,000</td>\n","      <td id=\"T_abdc0_row5_col2\" class=\"data row5 col2\" >N/A</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row6\" class=\"row_heading level0 row6\" >OOP Exclusions</th>\n","      <td id=\"T_abdc0_row6_col0\" class=\"data row6 col0\" >Premiums, balance-billing charges, and health care this plan doesn't cover.</td>\n","      <td id=\"T_abdc0_row6_col1\" class=\"data row6 col1\" >Pre-Authorization Penalties, \n","Premiums, balance-billing \n","charges, and health care this plan \n","doesn't cover.</td>\n","      <td id=\"T_abdc0_row6_col2\" class=\"data row6 col2\" >Premiums, balance-billing charges, and health care this plan doesn't cover.</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row7\" class=\"row_heading level0 row7\" >Issuing State</th>\n","      <td id=\"T_abdc0_row7_col0\" class=\"data row7 col0\" >CA</td>\n","      <td id=\"T_abdc0_row7_col1\" class=\"data row7 col1\" >CA</td>\n","      <td id=\"T_abdc0_row7_col2\" class=\"data row7 col2\" >CA</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row8\" class=\"row_heading level0 row8\" >Start Date</th>\n","      <td id=\"T_abdc0_row8_col0\" class=\"data row8 col0\" >07/01/2024</td>\n","      <td id=\"T_abdc0_row8_col1\" class=\"data row8 col1\" >07/01/2024</td>\n","      <td id=\"T_abdc0_row8_col2\" class=\"data row8 col2\" >07/01/2024</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_abdc0_level0_row9\" class=\"row_heading level0 row9\" >End Date</th>\n","      <td id=\"T_abdc0_row9_col0\" class=\"data row9 col0\" >06/30/2025</td>\n","      <td id=\"T_abdc0_row9_col1\" class=\"data row9 col1\" >06/30/2025</td>\n","      <td id=\"T_abdc0_row9_col2\" class=\"data row9 col2\" >06/30/2025</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7a6059f56f10>"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","import io\n","import json\n","import contextlib\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","from IPython.display import display\n","\n","def compare_plans(store, llm):\n","    \"\"\"\n","    Runs RAG + LLM per plan (with tqdm), then returns\n","    a cleaned & styled DataFrame (no .pdf suffixes, Yes/No,\n","    normalized dates) with a caption title.\n","    \"\"\"\n","    buf = io.StringIO()\n","    with contextlib.redirect_stdout(buf):\n","        docs  = multi_query_retrieval(store)\n","        plans = group_by_plan(docs)\n","\n","    results = {}\n","    for pid, chunks in tqdm(plans.items(), desc=\"Processing plans\", unit=\"plan\"):\n","        ctx    = \"\\n\\n---\\n\\n\".join(d.page_content for d in chunks)\n","        prompt = build_prompt(ctx)\n","        resp   = llm.invoke(prompt)\n","        clean  = resp.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n","        data   = json.loads(clean)\n","        for e in data.get(\"out_of_pocket_max_values\", []):\n","            if e.get(\"value\") is None:\n","                e[\"value\"] = \"N/A\"\n","        results[pid] = data\n","\n","    # Define rows\n","    sample      = next(iter(results.values()))\n","    limit_types = [v[\"limit_type\"] for v in sample[\"out_of_pocket_max_values\"]]\n","\n","    row_defs = [\n","        (\"Plan Name\",   lambda d: d.get(\"carrierPlanName\", \"\")),\n","        (\"HSA Offered\", lambda d: str(d.get(\"hsaOffered\", \"\"))),\n","    ]\n","    for lt in limit_types:\n","        label = lt.replace(\"Individual/Person\", \"OOP Max (Indv)\") \\\n","                  .replace(\"Family\", \"OOP Max (Fam)\")\n","        row_defs.append((\n","            label,\n","            lambda d, lt=lt: next(\n","                (x[\"value\"] for x in d[\"out_of_pocket_max_values\"] if x[\"limit_type\"]==lt),\n","                \"\"\n","            )\n","        ))\n","    row_defs += [\n","        (\"OOP Exclusions\", lambda d: d.get(\"out_of_pocket_exclusions\", \"\")),\n","        (\"Issuing State\",  lambda d: d.get(\"issuingState\", \"\")),\n","        (\"Start Date\",     lambda d: d.get(\"startDate\", \"\")),\n","        (\"End Date\",       lambda d: d.get(\"endDate\", \"\")),\n","    ]\n","\n","    # Build DataFrame\n","    plan_ids = list(results.keys())\n","    data     = {\n","        pid: [ extractor(results[pid]) for _, extractor in row_defs ]\n","        for pid in plan_ids\n","    }\n","    index = [ label for label, _ in row_defs ]\n","    df    = pd.DataFrame(data, index=index)\n","\n","    # 1) Strip .pdf from headers\n","    df.columns = [ os.path.splitext(c)[0] for c in df.columns ]\n","    df.columns.name = \"Plan\"\n","    df.index.name   = \"\"\n","\n","    # 2) Fallback Plan Name â†’ filename (no .pdf)\n","    df.loc[\"Plan Name\"] = [\n","        name if name else os.path.splitext(pid)[0]\n","        for name, pid in zip(df.loc[\"Plan Name\"], plan_ids)\n","    ]\n","\n","    # 3) True/False â†’ Yes/No\n","    df.replace({\"True\": \"Yes\", \"False\": \"No\"}, inplace=True)\n","\n","    # 4) Normalize dates to MM/DD/YYYY\n","    def fmt_date(x):\n","        try:\n","            dt = pd.to_datetime(x, errors=\"coerce\")\n","            return dt.strftime(\"%m/%d/%Y\") if not pd.isna(dt) else x\n","        except:\n","            return x\n","\n","    df.loc[\"Start Date\"] = df.loc[\"Start Date\"].apply(fmt_date)\n","    df.loc[\"End Date\"]   = df.loc[\"End Date\"].apply(fmt_date)\n","\n","    # 5) Style with caption\n","    styled = (\n","        df.style\n","          .set_caption(\"Outâ€‘ofâ€‘Pocket Expensese & Plan Details Comparison\")\n","          .set_properties(**{\"text-align\": \"center\"})\n","          .set_table_styles([\n","              {\"selector\": \"th\",      \"props\": [(\"text-align\", \"center\")]},\n","              {\"selector\": \"caption\",\"props\": [\n","                  (\"font-weight\",    \"bold\"),\n","                  (\"font-size\", \"1.25rem\"),\n","                  (\"padding-bottom\",\"0.75em\")\n","              ]}\n","          ])\n","    )\n","    return styled\n","\n","def main():\n","    # Set these to be a little more relaxed for better NLP, yet still deterministic\n","    TEMP, TOP_P, TOP_K = 0.2, 0.9, 8\n","\n","    # Initialize\n","    api_key  = load_api_key()\n","    embedder = init_embedder(api_key)\n","    store    = load_store(embedder)\n","    llm      = init_llm(api_key, temperature=TEMP, top_p=TOP_P, top_k=TOP_K)\n","    \n","    # Echo the prompt\n","    print(\"\\nğŸ§‘â€ğŸ’¼ User: Please compare all three plans and provide outâ€‘ofâ€‘pocketâ€‘expense summary in table format.\\n\")\n","\n","    # Compare and display\n","    print(f\"ğŸ¤– ChatBot: Let me look into that for you...\")\n","    styled_df = compare_plans(store, llm)\n","    print()\n","    display(styled_df)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","id":"ecd8170e","metadata":{"papermill":{"duration":0.007038,"end_time":"2025-04-18T19:46:28.582354","exception":false,"start_time":"2025-04-18T19:46:28.575316","status":"completed"},"tags":[]},"source":["## Stay in touch\n","\n","As I continue to explore Google AIs capabilities for use cases at my own company, I encourage you to stay in touch.\n","\n","You can follow me through my [substack](https://daisyhealthcare.substack.com/) and on [LinkedIn](https://www.linkedin.com/in/lsacco/)."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":7166721,"sourceId":11456426,"sourceType":"datasetVersion"}],"dockerImageVersionId":31012,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":102.38318,"end_time":"2025-04-18T19:46:30.215697","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-18T19:44:47.832517","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"043c07436fdb4f9289ed8c229e5b738d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0db84d426a7143ddb5d7416ba671d62d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a529b5c18a1b43f6a35a184571988a5e","IPY_MODEL_7b11d989bbd8482abaf9b157fabc6080","IPY_MODEL_9330c3c4c42d4812b9ef97402a772bad"],"layout":"IPY_MODEL_25ea928045e7497f8d1d357a3f66d124","tabbable":null,"tooltip":null}},"25ea928045e7497f8d1d357a3f66d124":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3502400d6a8a4e279b621e1bdccb0b94":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38df9f18e6654c55a5fb02ea10c1cddf":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6bf5a0478a634cab93ca451df6d31c1d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b11d989bbd8482abaf9b157fabc6080":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_3502400d6a8a4e279b621e1bdccb0b94","max":3.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_38df9f18e6654c55a5fb02ea10c1cddf","tabbable":null,"tooltip":null,"value":3.0}},"9330c3c4c42d4812b9ef97402a772bad":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_6bf5a0478a634cab93ca451df6d31c1d","placeholder":"â€‹","style":"IPY_MODEL_eccf49f3891a4799b5c7955de8daf2c0","tabbable":null,"tooltip":null,"value":"â€‡3/3â€‡[00:05&lt;00:00,â€‡â€‡1.93s/plan]"}},"a529b5c18a1b43f6a35a184571988a5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_043c07436fdb4f9289ed8c229e5b738d","placeholder":"â€‹","style":"IPY_MODEL_b005262036f14511b732cd182f741c7d","tabbable":null,"tooltip":null,"value":"Processingâ€‡plans:â€‡100%"}},"b005262036f14511b732cd182f741c7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"eccf49f3891a4799b5c7955de8daf2c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}